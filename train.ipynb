{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "c52b91c0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c52b91c0",
        "outputId": "ebd44a54-bc0c-46d0-ec1d-29a942e01561"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.3.0+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.25.2)\n",
            "Collecting timm==0.5.4\n",
            "  Downloading timm-0.5.4-py3-none-any.whl (431 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m431.5/431.5 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.4)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from timm==0.5.4) (0.18.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.3.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.40-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m43.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->timm==0.5.4) (9.4.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, timm\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.40 nvidia-nvtx-cu12-12.1.105 timm-0.5.4\n"
          ]
        }
      ],
      "source": [
        "! pip install torch numpy timm==0.5.4 tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "383c6887",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "383c6887",
        "outputId": "32f77081-0022-4205-b7e8-851be57ff13d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'Vision-Transformer'...\n",
            "remote: Enumerating objects: 20, done.\u001b[K\n",
            "remote: Counting objects: 100% (20/20), done.\u001b[K\n",
            "remote: Compressing objects: 100% (14/14), done.\u001b[K\n",
            "remote: Total 20 (delta 8), reused 18 (delta 6), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (20/20), 17.05 KiB | 1.71 MiB/s, done.\n",
            "Resolving deltas: 100% (8/8), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/tsungchiehchen/Vision-Transformer.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "78da3d77",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "78da3d77",
        "outputId": "1c4e1e08-a540-4489-b9fd-d632497e47af"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/Vision-Transformer\n"
          ]
        }
      ],
      "source": [
        "%cd ./Vision-Transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "e7507140",
      "metadata": {
        "id": "e7507140"
      },
      "outputs": [],
      "source": [
        "import argparse\n",
        "import datetime\n",
        "import os\n",
        "import sys\n",
        "import numpy as np\n",
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.backends.cudnn as cudnn\n",
        "\n",
        "from timm.models import create_model\n",
        "\n",
        "from engine import train_one_epoch, train_one_epoch_distillation, evaluate\n",
        "from utils import get_training_dataloader, get_test_dataloader\n",
        "import models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "234c6ce8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "234c6ce8",
        "outputId": "e21eed76-b6b3-4265-d691-6078cf240e1c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Creating model: vit_base_patch16_224\n",
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 170498071/170498071 [00:02<00:00, 79445281.30it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n",
            "number of params: 86567656\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        }
      ],
      "source": [
        "MEAN = (0.5070751592371323, 0.48654887331495095, 0.4409178433670343)\n",
        "STD = (0.2673342858792401, 0.2564384629170883, 0.27615047132568404)\n",
        "CHECKPOINT_PATH = './checkpoint'\n",
        "MODEL_NAME = 'vit_base_patch16_224'\n",
        "num_classes = 10\n",
        "EPOCHS = 5\n",
        "LR = 0.0001\n",
        "WD = 0.0\n",
        "shots = 1000\n",
        "\n",
        "print(f\"Creating model: {MODEL_NAME}\")\n",
        "model = create_model(\n",
        "        MODEL_NAME,\n",
        "        pretrained=False,\n",
        "        num_classes=10,\n",
        "        img_size=224)\n",
        "device = 'cuda:0' # device = 'cpu'\n",
        "model = model.to(device)\n",
        "\n",
        "cifar10_training_loader = get_training_dataloader(\n",
        "    MEAN,\n",
        "    STD,\n",
        "    num_workers=2,\n",
        "    batch_size=16,\n",
        "    shuffle=True,\n",
        "    shots=shots\n",
        ")\n",
        "\n",
        "assert (shots*num_classes == len(cifar10_training_loader.dataset))\n",
        "\n",
        "cifar10_test_loader = get_test_dataloader(\n",
        "    MEAN,\n",
        "    STD,\n",
        "    num_workers=4,\n",
        "    batch_size=256,\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=LR, weight_decay=WD)\n",
        "\n",
        "\n",
        "n_parameters = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print('number of params:', n_parameters)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "f51e2794",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f51e2794",
        "outputId": "1a6f5c56-b383-4aac-f881-6bb6793b75a4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Start training for 5 epochs\n",
            "Epoch: [1]  [  0/625]  eta: 0:28:48  loss: 7.2056 (7.2056)  time: 2.7654  data: 0.4869  max mem: 2096\n",
            "Epoch: [1]  [100/625]  eta: 0:01:44  loss: 2.2015 (2.4863)  time: 0.1713  data: 0.0057  max mem: 3065\n",
            "Epoch: [1]  [200/625]  eta: 0:01:19  loss: 2.1340 (2.3034)  time: 0.1701  data: 0.0055  max mem: 3065\n",
            "Epoch: [1]  [300/625]  eta: 0:00:59  loss: 2.0563 (2.2224)  time: 0.1778  data: 0.0084  max mem: 3065\n",
            "Epoch: [1]  [400/625]  eta: 0:00:40  loss: 2.1087 (2.1713)  time: 0.1774  data: 0.0065  max mem: 3065\n",
            "Epoch: [1]  [500/625]  eta: 0:00:22  loss: 1.8859 (2.1306)  time: 0.1793  data: 0.0054  max mem: 3065\n",
            "Epoch: [1]  [600/625]  eta: 0:00:04  loss: 1.9450 (2.1032)  time: 0.1898  data: 0.0100  max mem: 3065\n",
            "Epoch: [1]  [624/625]  eta: 0:00:00  loss: 2.0016 (2.0996)  time: 0.1834  data: 0.0051  max mem: 3065\n",
            "Epoch: [1] Total time: 0:01:53 (0.1819 s / it)\n",
            "Averaged stats: loss: 2.0016 (2.0996)\n",
            "Epoch: [2]  [  0/625]  eta: 0:03:33  loss: 2.1157 (2.1157)  time: 0.3417  data: 0.1587  max mem: 3065\n",
            "Epoch: [2]  [100/625]  eta: 0:01:40  loss: 1.8432 (1.9094)  time: 0.1923  data: 0.0085  max mem: 3065\n",
            "Epoch: [2]  [200/625]  eta: 0:01:20  loss: 1.8190 (1.8919)  time: 0.1830  data: 0.0053  max mem: 3065\n",
            "Epoch: [2]  [300/625]  eta: 0:01:01  loss: 1.8658 (1.8762)  time: 0.1825  data: 0.0054  max mem: 3065\n",
            "Epoch: [2]  [400/625]  eta: 0:00:42  loss: 1.7444 (1.8771)  time: 0.1865  data: 0.0079  max mem: 3065\n",
            "Epoch: [2]  [500/625]  eta: 0:00:23  loss: 1.8485 (1.8693)  time: 0.1840  data: 0.0053  max mem: 3065\n",
            "Epoch: [2]  [600/625]  eta: 0:00:04  loss: 1.7978 (1.8610)  time: 0.1920  data: 0.0105  max mem: 3065\n",
            "Epoch: [2]  [624/625]  eta: 0:00:00  loss: 1.8339 (1.8594)  time: 0.1836  data: 0.0055  max mem: 3065\n",
            "Epoch: [2] Total time: 0:01:56 (0.1868 s / it)\n",
            "Averaged stats: loss: 1.8339 (1.8594)\n",
            "Epoch: [3]  [  0/625]  eta: 0:03:24  loss: 1.9638 (1.9638)  time: 0.3265  data: 0.1391  max mem: 3065\n",
            "Epoch: [3]  [100/625]  eta: 0:01:37  loss: 1.7909 (1.7991)  time: 0.1826  data: 0.0055  max mem: 3065\n",
            "Epoch: [3]  [200/625]  eta: 0:01:18  loss: 1.6734 (1.7751)  time: 0.1843  data: 0.0066  max mem: 3065\n",
            "Epoch: [3]  [300/625]  eta: 0:01:00  loss: 1.6332 (1.7771)  time: 0.1824  data: 0.0054  max mem: 3065\n",
            "Epoch: [3]  [400/625]  eta: 0:00:41  loss: 1.8108 (1.8007)  time: 0.1913  data: 0.0105  max mem: 3065\n",
            "Epoch: [3]  [500/625]  eta: 0:00:23  loss: 1.8656 (1.7931)  time: 0.1840  data: 0.0054  max mem: 3065\n",
            "Epoch: [3]  [600/625]  eta: 0:00:04  loss: 1.7229 (1.7936)  time: 0.1863  data: 0.0079  max mem: 3065\n",
            "Epoch: [3]  [624/625]  eta: 0:00:00  loss: 1.6740 (1.7939)  time: 0.1855  data: 0.0065  max mem: 3065\n",
            "Epoch: [3] Total time: 0:01:56 (0.1858 s / it)\n",
            "Averaged stats: loss: 1.6740 (1.7939)\n",
            "Epoch: [4]  [  0/625]  eta: 0:03:36  loss: 1.2824 (1.2824)  time: 0.3457  data: 0.1420  max mem: 3065\n",
            "Epoch: [4]  [100/625]  eta: 0:01:38  loss: 1.7753 (1.7321)  time: 0.1835  data: 0.0059  max mem: 3065\n",
            "Epoch: [4]  [200/625]  eta: 0:01:19  loss: 1.6732 (1.7365)  time: 0.1914  data: 0.0103  max mem: 3065\n",
            "Epoch: [4]  [300/625]  eta: 0:01:00  loss: 1.6920 (1.7324)  time: 0.1826  data: 0.0054  max mem: 3065\n",
            "Epoch: [4]  [400/625]  eta: 0:00:41  loss: 1.6332 (1.7279)  time: 0.1845  data: 0.0071  max mem: 3065\n",
            "Epoch: [4]  [500/625]  eta: 0:00:23  loss: 1.7922 (1.7346)  time: 0.1839  data: 0.0061  max mem: 3065\n",
            "Epoch: [4]  [600/625]  eta: 0:00:04  loss: 1.6619 (1.7348)  time: 0.1832  data: 0.0060  max mem: 3065\n",
            "Epoch: [4]  [624/625]  eta: 0:00:00  loss: 1.7109 (1.7339)  time: 0.1882  data: 0.0091  max mem: 3065\n",
            "Epoch: [4] Total time: 0:01:55 (0.1855 s / it)\n",
            "Averaged stats: loss: 1.7109 (1.7339)\n",
            "Epoch: [5]  [  0/625]  eta: 0:03:26  loss: 1.4502 (1.4502)  time: 0.3312  data: 0.1415  max mem: 3065\n",
            "Epoch: [5]  [100/625]  eta: 0:01:37  loss: 1.5983 (1.6231)  time: 0.1826  data: 0.0054  max mem: 3065\n",
            "Epoch: [5]  [200/625]  eta: 0:01:19  loss: 1.7797 (1.6652)  time: 0.1847  data: 0.0058  max mem: 3065\n",
            "Epoch: [5]  [300/625]  eta: 0:01:00  loss: 1.7249 (1.6864)  time: 0.1837  data: 0.0058  max mem: 3065\n",
            "Epoch: [5]  [400/625]  eta: 0:00:41  loss: 1.6766 (1.6921)  time: 0.1832  data: 0.0054  max mem: 3065\n",
            "Epoch: [5]  [500/625]  eta: 0:00:23  loss: 1.7141 (1.6965)  time: 0.1891  data: 0.0090  max mem: 3065\n",
            "Epoch: [5]  [600/625]  eta: 0:00:04  loss: 1.5961 (1.6943)  time: 0.1829  data: 0.0058  max mem: 3065\n",
            "Epoch: [5]  [624/625]  eta: 0:00:00  loss: 1.6461 (1.6918)  time: 0.1839  data: 0.0059  max mem: 3065\n",
            "Epoch: [5] Total time: 0:01:55 (0.1854 s / it)\n",
            "Averaged stats: loss: 1.6461 (1.6918)\n",
            "Test:  [ 0/40]  eta: 0:03:07  loss: 1.6862 (1.6862)  acc1: 40.6250 (40.6250)  acc5: 85.1562 (85.1562)  time: 4.6876  data: 3.2015  max mem: 3158\n",
            "Test:  [20/40]  eta: 0:00:24  loss: 1.7490 (1.7590)  acc1: 36.3281 (35.9189)  acc5: 88.6719 (88.4301)  time: 1.0772  data: 0.0756  max mem: 3158\n",
            "Test:  [39/40]  eta: 0:00:01  loss: 1.7465 (1.7480)  acc1: 36.7188 (36.1300)  acc5: 88.6719 (88.0300)  time: 1.0213  data: 0.0639  max mem: 3158\n",
            "Test: Total time: 0:00:45 (1.1429 s / it)\n",
            "* Acc@1 36.130 Acc@5 88.030 loss 1.748\n",
            "Accuracy of the network on the 40 test images: 36.1%\n"
          ]
        }
      ],
      "source": [
        "print(f\"Start training for {EPOCHS} epochs\")\n",
        "\n",
        "for epoch in range(1, EPOCHS+1):\n",
        "    train_stats = train_one_epoch(\n",
        "        model, criterion, cifar10_training_loader,\n",
        "        optimizer, device, epoch)\n",
        "    if epoch % 10 == 9:\n",
        "        test_stats = evaluate(cifar10_test_loader, model, criterion, device)\n",
        "        print(f\"Accuracy of the network on the {len(cifar10_test_loader)} test images: {test_stats['acc1']:.1f}%\")\n",
        "\n",
        "test_stats = evaluate(cifar10_test_loader, model, criterion, device)\n",
        "print(f\"Accuracy of the network on the {len(cifar10_test_loader)} test images: {test_stats['acc1']:.1f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "2d48bd9a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2d48bd9a",
        "outputId": "b4d85494-6205-403f-9aaf-db7e39fbc453"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test:  [ 0/40]  eta: 0:01:58  loss: 1.6862 (1.6862)  acc1: 40.6250 (40.6250)  acc5: 85.1562 (85.1562)  time: 2.9730  data: 1.8310  max mem: 3158\n",
            "Test:  [20/40]  eta: 0:00:23  loss: 1.7490 (1.7590)  acc1: 36.3281 (35.9189)  acc5: 88.6719 (88.4301)  time: 1.0917  data: 0.0786  max mem: 3158\n",
            "Test:  [39/40]  eta: 0:00:01  loss: 1.7465 (1.7480)  acc1: 36.7188 (36.1300)  acc5: 88.6719 (88.0300)  time: 1.0233  data: 0.0669  max mem: 3158\n",
            "Test: Total time: 0:00:44 (1.1050 s / it)\n",
            "* Acc@1 36.130 Acc@5 88.030 loss 1.748\n",
            "Throughput: 226.21906627248507\n"
          ]
        }
      ],
      "source": [
        "# Calculate througput\n",
        "start_time = time.time()\n",
        "test_stats = evaluate(cifar10_test_loader, model, criterion, device)\n",
        "end_time = time.time()\n",
        "num_samples = len(cifar10_test_loader.dataset)\n",
        "throughput = num_samples / (end_time - start_time)\n",
        "print(\"Throughput: {}\".format(throughput))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "61fdc53f",
      "metadata": {
        "id": "61fdc53f"
      },
      "source": [
        "# Q2 Fine-tuning Pretrained ViT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "ee8523be",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ee8523be",
        "outputId": "f27399e5-7f2e-470b-b050-e8834ab51c10"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Creating model: vit_base_patch16_224\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading: \"https://dl.fbaipublicfiles.com/deit/deit_base_patch16_224-b5f2ef4d.pth\" to /root/.cache/torch/hub/checkpoints/deit_base_patch16_224-b5f2ef4d.pth\n",
            "100%|██████████| 330M/330M [00:05<00:00, 59.9MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "number of params: 85806346\n"
          ]
        }
      ],
      "source": [
        "MEAN = (0.5070751592371323, 0.48654887331495095, 0.4409178433670343)\n",
        "STD = (0.2673342858792401, 0.2564384629170883, 0.27615047132568404)\n",
        "CHECKPOINT_PATH = './checkpoint'\n",
        "MODEL_NAME = 'vit_base_patch16_224'\n",
        "num_classes = 10\n",
        "EPOCHS = 5\n",
        "LR = 0.0001\n",
        "WD = 0.0\n",
        "shots = 1000\n",
        "\n",
        "print(f\"Creating model: {MODEL_NAME}\")\n",
        "model = create_model(\n",
        "        MODEL_NAME,\n",
        "        pretrained=True,\n",
        "        num_classes=10,\n",
        "        img_size=224)\n",
        "device = 'cuda:0' # device = 'cpu'\n",
        "model = model.to(device)\n",
        "\n",
        "cifar10_training_loader = get_training_dataloader(\n",
        "    MEAN,\n",
        "    STD,\n",
        "    num_workers=2,\n",
        "    batch_size=16,\n",
        "    shuffle=True,\n",
        "    shots=shots\n",
        ")\n",
        "\n",
        "assert (shots*num_classes == len(cifar10_training_loader.dataset))\n",
        "\n",
        "cifar10_test_loader = get_test_dataloader(\n",
        "    MEAN,\n",
        "    STD,\n",
        "    num_workers=4,\n",
        "    batch_size=256,\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=LR, weight_decay=WD)\n",
        "\n",
        "\n",
        "n_parameters = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print('number of params:', n_parameters)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "f881d0c0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f881d0c0",
        "outputId": "876be38a-34d6-4708-a6ec-32cf23fa72db"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Start training for 5 epochs\n",
            "Epoch: [1]  [  0/625]  eta: 0:04:01  loss: 2.3790 (2.3790)  time: 0.3858  data: 0.1525  max mem: 3158\n",
            "Epoch: [1]  [100/625]  eta: 0:01:38  loss: 2.0979 (2.2390)  time: 0.1886  data: 0.0055  max mem: 3158\n",
            "Epoch: [1]  [200/625]  eta: 0:01:20  loss: 2.0896 (2.1396)  time: 0.1938  data: 0.0096  max mem: 3158\n",
            "Epoch: [1]  [300/625]  eta: 0:01:01  loss: 1.8388 (2.0790)  time: 0.1859  data: 0.0053  max mem: 3158\n",
            "Epoch: [1]  [400/625]  eta: 0:00:42  loss: 1.7095 (2.0115)  time: 0.1875  data: 0.0071  max mem: 3158\n",
            "Epoch: [1]  [500/625]  eta: 0:00:23  loss: 1.5865 (1.9341)  time: 0.1861  data: 0.0057  max mem: 3158\n",
            "Epoch: [1]  [600/625]  eta: 0:00:04  loss: 1.4219 (1.8547)  time: 0.1886  data: 0.0058  max mem: 3158\n",
            "Epoch: [1]  [624/625]  eta: 0:00:00  loss: 1.4646 (1.8390)  time: 0.1900  data: 0.0065  max mem: 3158\n",
            "Epoch: [1] Total time: 0:01:58 (0.1890 s / it)\n",
            "Averaged stats: loss: 1.4646 (1.8390)\n",
            "Epoch: [2]  [  0/625]  eta: 0:03:39  loss: 1.2923 (1.2923)  time: 0.3512  data: 0.1692  max mem: 3158\n",
            "Epoch: [2]  [100/625]  eta: 0:01:40  loss: 1.1733 (1.2888)  time: 0.1884  data: 0.0053  max mem: 3158\n",
            "Epoch: [2]  [200/625]  eta: 0:01:21  loss: 1.0698 (1.2387)  time: 0.1939  data: 0.0105  max mem: 3158\n",
            "Epoch: [2]  [300/625]  eta: 0:01:01  loss: 0.9309 (1.1953)  time: 0.1869  data: 0.0053  max mem: 3158\n",
            "Epoch: [2]  [400/625]  eta: 0:00:42  loss: 1.0493 (1.1552)  time: 0.1945  data: 0.0101  max mem: 3158\n",
            "Epoch: [2]  [500/625]  eta: 0:00:23  loss: 0.9067 (1.1138)  time: 0.1880  data: 0.0051  max mem: 3158\n",
            "Epoch: [2]  [600/625]  eta: 0:00:04  loss: 0.9747 (1.0836)  time: 0.1916  data: 0.0073  max mem: 3158\n",
            "Epoch: [2]  [624/625]  eta: 0:00:00  loss: 0.9592 (1.0760)  time: 0.1878  data: 0.0054  max mem: 3158\n",
            "Epoch: [2] Total time: 0:01:59 (0.1904 s / it)\n",
            "Averaged stats: loss: 0.9592 (1.0760)\n",
            "Epoch: [3]  [  0/625]  eta: 0:03:41  loss: 0.7230 (0.7230)  time: 0.3548  data: 0.1660  max mem: 3158\n",
            "Epoch: [3]  [100/625]  eta: 0:01:39  loss: 0.7007 (0.7844)  time: 0.1872  data: 0.0054  max mem: 3158\n",
            "Epoch: [3]  [200/625]  eta: 0:01:20  loss: 0.7821 (0.7712)  time: 0.1894  data: 0.0068  max mem: 3158\n",
            "Epoch: [3]  [300/625]  eta: 0:01:01  loss: 0.7675 (0.7505)  time: 0.1877  data: 0.0057  max mem: 3158\n",
            "Epoch: [3]  [400/625]  eta: 0:00:42  loss: 0.5703 (0.7351)  time: 0.1893  data: 0.0064  max mem: 3158\n",
            "Epoch: [3]  [500/625]  eta: 0:00:23  loss: 0.5558 (0.7201)  time: 0.1869  data: 0.0054  max mem: 3158\n",
            "Epoch: [3]  [600/625]  eta: 0:00:04  loss: 0.5964 (0.7125)  time: 0.1928  data: 0.0092  max mem: 3158\n",
            "Epoch: [3]  [624/625]  eta: 0:00:00  loss: 0.6316 (0.7110)  time: 0.1875  data: 0.0052  max mem: 3158\n",
            "Epoch: [3] Total time: 0:01:58 (0.1896 s / it)\n",
            "Averaged stats: loss: 0.6316 (0.7110)\n",
            "Epoch: [4]  [  0/625]  eta: 0:03:48  loss: 0.6848 (0.6848)  time: 0.3655  data: 0.1727  max mem: 3158\n",
            "Epoch: [4]  [100/625]  eta: 0:01:39  loss: 0.4594 (0.5009)  time: 0.1866  data: 0.0056  max mem: 3158\n",
            "Epoch: [4]  [200/625]  eta: 0:01:20  loss: 0.5463 (0.5158)  time: 0.1900  data: 0.0077  max mem: 3158\n",
            "Epoch: [4]  [300/625]  eta: 0:01:01  loss: 0.4741 (0.5190)  time: 0.1865  data: 0.0060  max mem: 3158\n",
            "Epoch: [4]  [400/625]  eta: 0:00:42  loss: 0.4251 (0.5156)  time: 0.1938  data: 0.0111  max mem: 3158\n",
            "Epoch: [4]  [500/625]  eta: 0:00:23  loss: 0.4471 (0.5166)  time: 0.1867  data: 0.0056  max mem: 3158\n",
            "Epoch: [4]  [600/625]  eta: 0:00:04  loss: 0.5629 (0.5153)  time: 0.1912  data: 0.0091  max mem: 3158\n",
            "Epoch: [4]  [624/625]  eta: 0:00:00  loss: 0.5342 (0.5171)  time: 0.1866  data: 0.0052  max mem: 3158\n",
            "Epoch: [4] Total time: 0:01:58 (0.1890 s / it)\n",
            "Averaged stats: loss: 0.5342 (0.5171)\n",
            "Epoch: [5]  [  0/625]  eta: 0:03:51  loss: 0.4008 (0.4008)  time: 0.3707  data: 0.1841  max mem: 3158\n",
            "Epoch: [5]  [100/625]  eta: 0:01:40  loss: 0.2556 (0.3831)  time: 0.1874  data: 0.0062  max mem: 3158\n",
            "Epoch: [5]  [200/625]  eta: 0:01:20  loss: 0.3414 (0.3919)  time: 0.1874  data: 0.0067  max mem: 3158\n",
            "Epoch: [5]  [300/625]  eta: 0:01:01  loss: 0.4070 (0.3873)  time: 0.1866  data: 0.0053  max mem: 3158\n",
            "Epoch: [5]  [400/625]  eta: 0:00:42  loss: 0.3446 (0.3835)  time: 0.1916  data: 0.0100  max mem: 3158\n",
            "Epoch: [5]  [500/625]  eta: 0:00:23  loss: 0.3451 (0.3870)  time: 0.1853  data: 0.0052  max mem: 3158\n",
            "Epoch: [5]  [600/625]  eta: 0:00:04  loss: 0.4126 (0.3862)  time: 0.1933  data: 0.0105  max mem: 3158\n",
            "Epoch: [5]  [624/625]  eta: 0:00:00  loss: 0.3600 (0.3854)  time: 0.1858  data: 0.0054  max mem: 3158\n",
            "Epoch: [5] Total time: 0:01:57 (0.1888 s / it)\n",
            "Averaged stats: loss: 0.3600 (0.3854)\n",
            "Test:  [ 0/40]  eta: 0:02:18  loss: 0.6622 (0.6622)  acc1: 77.7344 (77.7344)  acc5: 98.4375 (98.4375)  time: 3.4508  data: 2.3775  max mem: 3158\n",
            "Test:  [20/40]  eta: 0:00:24  loss: 0.5921 (0.5938)  acc1: 80.0781 (80.3571)  acc5: 99.2188 (99.0699)  time: 1.0996  data: 0.0779  max mem: 3158\n",
            "Test:  [39/40]  eta: 0:00:01  loss: 0.5921 (0.5866)  acc1: 80.4688 (80.3500)  acc5: 99.2188 (99.1700)  time: 1.0202  data: 0.0550  max mem: 3158\n",
            "Test: Total time: 0:00:44 (1.1238 s / it)\n",
            "* Acc@1 80.350 Acc@5 99.170 loss 0.587\n",
            "Accuracy of the network on the 40 test images: 80.3%\n"
          ]
        }
      ],
      "source": [
        "print(f\"Start training for {EPOCHS} epochs\")\n",
        "\n",
        "for epoch in range(1, EPOCHS+1):\n",
        "    train_stats = train_one_epoch(\n",
        "        model, criterion, cifar10_training_loader,\n",
        "        optimizer, device, epoch)\n",
        "    if epoch % 10 == 9:\n",
        "        test_stats = evaluate(cifar10_test_loader, model, criterion, device)\n",
        "        print(f\"Accuracy of the network on the {len(cifar10_test_loader)} test images: {test_stats['acc1']:.1f}%\")\n",
        "\n",
        "test_stats = evaluate(cifar10_test_loader, model, criterion, device)\n",
        "print(f\"Accuracy of the network on the {len(cifar10_test_loader)} test images: {test_stats['acc1']:.1f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "e323d040",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e323d040",
        "outputId": "b35ef800-3dcb-4413-9c2a-e2d01c9466dc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test:  [ 0/40]  eta: 0:02:29  loss: 0.6622 (0.6622)  acc1: 77.7344 (77.7344)  acc5: 98.4375 (98.4375)  time: 3.7399  data: 2.5347  max mem: 3158\n",
            "Test:  [20/40]  eta: 0:00:25  loss: 0.5921 (0.5938)  acc1: 80.0781 (80.3571)  acc5: 99.2188 (99.0699)  time: 1.1569  data: 0.1107  max mem: 3158\n",
            "Test:  [39/40]  eta: 0:00:01  loss: 0.5921 (0.5866)  acc1: 80.4688 (80.3500)  acc5: 99.2188 (99.1700)  time: 1.0263  data: 0.0535  max mem: 3364\n",
            "Test: Total time: 0:00:46 (1.1643 s / it)\n",
            "* Acc@1 80.350 Acc@5 99.170 loss 0.587\n",
            "Throughput: 214.69383078072903\n"
          ]
        }
      ],
      "source": [
        "# Calculate througput\n",
        "start_time = time.time()\n",
        "test_stats = evaluate(cifar10_test_loader, model, criterion, device)\n",
        "end_time = time.time()\n",
        "num_samples = len(cifar10_test_loader.dataset)\n",
        "throughput = num_samples / (end_time - start_time)\n",
        "print(\"Throughput: {}\".format(throughput))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a93beb48",
      "metadata": {
        "id": "a93beb48"
      },
      "source": [
        "# Q3 ViT model on a small device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dcb0666d",
      "metadata": {
        "id": "dcb0666d"
      },
      "outputs": [],
      "source": [
        "MEAN = (0.5070751592371323, 0.48654887331495095, 0.4409178433670343)\n",
        "STD = (0.2673342858792401, 0.2564384629170883, 0.27615047132568404)\n",
        "CHECKPOINT_PATH = './checkpoint'\n",
        "MODEL_NAME = 'vit_tiny_patch16_224'\n",
        "num_classes = 10\n",
        "EPOCHS = 5\n",
        "LR = 0.0001\n",
        "WD = 0.0\n",
        "shots = 1000\n",
        "\n",
        "print(f\"Creating model: {MODEL_NAME}\")\n",
        "model = create_model(\n",
        "        MODEL_NAME,\n",
        "        pretrained=True,\n",
        "        num_classes=10,\n",
        "        img_size=224)\n",
        "device = 'cuda:0' # device = 'cpu'\n",
        "model = model.to(device)\n",
        "\n",
        "cifar10_training_loader = get_training_dataloader(\n",
        "    MEAN,\n",
        "    STD,\n",
        "    num_workers=2,\n",
        "    batch_size=16,\n",
        "    shuffle=True,\n",
        "    shots=shots\n",
        ")\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=LR, weight_decay=WD)\n",
        "\n",
        "\n",
        "n_parameters = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print('number of params:', n_parameters)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "138b86de",
      "metadata": {
        "id": "138b86de"
      },
      "outputs": [],
      "source": [
        "print(f\"Start training for {EPOCHS} epochs\")\n",
        "\n",
        "for epoch in range(1, EPOCHS+1):\n",
        "    train_stats = train_one_epoch(\n",
        "        model, criterion, cifar10_training_loader,\n",
        "        optimizer, device, epoch)\n",
        "    if epoch % 10 == 9:\n",
        "        test_stats = evaluate(cifar10_test_loader, model, criterion, device)\n",
        "        print(f\"Accuracy of the network on the {len(cifar10_test_loader)} test images: {test_stats['acc1']:.1f}%\")\n",
        "\n",
        "test_stats = evaluate(cifar10_test_loader, model, criterion, device)\n",
        "print(f\"Accuracy of the network on the {len(cifar10_test_loader)} test images: {test_stats['acc1']:.1f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "199e33f5",
      "metadata": {
        "id": "199e33f5"
      },
      "outputs": [],
      "source": [
        "# Calculate througput\n",
        "start_time = time.time()\n",
        "test_stats = evaluate(cifar10_test_loader, model, criterion, device)\n",
        "end_time = time.time()\n",
        "num_samples = len(cifar10_test_loader.dataset)\n",
        "throughput = num_samples / (end_time - start_time)\n",
        "print(\"Throughput: {}\".format(throughput))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "33816cf2",
      "metadata": {
        "id": "33816cf2"
      },
      "source": [
        "# Q4 Knowledge Distillation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "caaf4f8a",
      "metadata": {
        "id": "caaf4f8a"
      },
      "outputs": [],
      "source": [
        "# Step 1: Train the teacher\n",
        "\n",
        "MODEL_NAME = 'vit_base_patch16_224'\n",
        "num_classes = 10\n",
        "EPOCHS = 5\n",
        "LR = 0.0001\n",
        "WD = 0.0\n",
        "\n",
        "print(f\"Creating model: {MODEL_NAME}\")\n",
        "teacher = create_model(\n",
        "        MODEL_NAME,\n",
        "        pretrained=True,\n",
        "        num_classes=10,\n",
        "        img_size=224)\n",
        "device = 'cuda:0' # device = 'cpu'\n",
        "teacher = teacher.to(device)\n",
        "\n",
        "\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(teacher.parameters(), lr=LR, weight_decay=WD)\n",
        "\n",
        "\n",
        "n_parameters = sum(p.numel() for p in teacher.parameters() if p.requires_grad)\n",
        "print('number of params:', n_parameters)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6c392f94",
      "metadata": {
        "id": "6c392f94"
      },
      "outputs": [],
      "source": [
        "print(f\"Start training for {EPOCHS} epochs\")\n",
        "\n",
        "for epoch in range(1, EPOCHS+1):\n",
        "    train_stats = train_one_epoch(\n",
        "        teacher, criterion, cifar10_training_loader,\n",
        "        optimizer, device, epoch)\n",
        "    if epoch % 10 == 9:\n",
        "        test_stats = evaluate(cifar10_test_loader, teacher, criterion, device)\n",
        "        print(f\"Accuracy of the network on the {len(cifar10_test_loader)} test images: {test_stats['acc1']:.1f}%\")\n",
        "\n",
        "test_stats = evaluate(cifar10_test_loader, teacher, criterion, device)\n",
        "print(f\"Accuracy of the network on the {len(cifar10_test_loader)} test images: {test_stats['acc1']:.1f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9ab509fd",
      "metadata": {
        "id": "9ab509fd"
      },
      "outputs": [],
      "source": [
        "# save finetuned teacher model\n",
        "torch.save(teacher.state_dict(), './teacher.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "69587aa8",
      "metadata": {
        "id": "69587aa8"
      },
      "outputs": [],
      "source": [
        "\n",
        "teacher = create_model(\n",
        "        'vit_base_patch16_224',\n",
        "        pretrained=True,\n",
        "        num_classes=10,\n",
        "        img_size=224)\n",
        "device = 'cuda:0' # device = 'cpu'\n",
        "teacher = teacher.to(device)\n",
        "teacher.load_state_dict(torch.load('./teacher.pth'))\n",
        "\n",
        "test_stats = evaluate(cifar10_test_loader, teacher, criterion, device)\n",
        "print(f\"Accuracy of the network on the {len(cifar10_test_loader)} test images: {test_stats['acc1']:.1f}%\")\n",
        "\n",
        "# Train the student\n",
        "for p in teacher.parameters():\n",
        "    p.requires_grad = False\n",
        "\n",
        "MODEL_NAME = 'vit_tiny_patch16_224'\n",
        "\n",
        "model = create_model(\n",
        "        MODEL_NAME,\n",
        "        pretrained=True,\n",
        "        num_classes=10,\n",
        "        img_size=224)\n",
        "device = 'cuda:0' # device = 'cpu'\n",
        "model = model.to(device)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=LR, weight_decay=WD)\n",
        "\n",
        "\n",
        "n_parameters = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print('number of params:', n_parameters)\n",
        "\n",
        "\n",
        "print(f\"Start training for {EPOCHS} epochs\")\n",
        "\n",
        "for epoch in range(1, EPOCHS+1):\n",
        "    train_stats = train_one_epoch_distillation(\n",
        "        teacher, model, criterion, cifar10_training_loader,\n",
        "        optimizer, device, epoch, alpha=2.0, temp=1.0)\n",
        "    if epoch % 2 == 1:\n",
        "        test_stats = evaluate(cifar10_test_loader, model, criterion, device)\n",
        "        print(f\"Accuracy of the network on the {len(cifar10_test_loader)} test images: {test_stats['acc1']:.1f}%\")\n",
        "\n",
        "test_stats = evaluate(cifar10_test_loader, model, criterion, device)\n",
        "print(f\"Accuracy of the network on the {len(cifar10_test_loader)} test images: {test_stats['acc1']:.1f}%\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
