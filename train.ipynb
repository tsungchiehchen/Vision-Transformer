{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c52b91c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /opt/homebrew/lib/python3.11/site-packages (2.1.1)\n",
      "Requirement already satisfied: numpy in /opt/homebrew/lib/python3.11/site-packages (1.24.3)\n",
      "Requirement already satisfied: timm==0.5.4 in /opt/homebrew/lib/python3.11/site-packages (0.5.4)\n",
      "Requirement already satisfied: tqdm in /opt/homebrew/lib/python3.11/site-packages (4.66.2)\n",
      "Requirement already satisfied: torchvision in /opt/homebrew/lib/python3.11/site-packages (from timm==0.5.4) (0.16.1)\n",
      "Requirement already satisfied: filelock in /opt/homebrew/lib/python3.11/site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in /opt/homebrew/lib/python3.11/site-packages (from torch) (4.8.0)\n",
      "Requirement already satisfied: sympy in /opt/homebrew/lib/python3.11/site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/homebrew/lib/python3.11/site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in /opt/homebrew/lib/python3.11/site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /opt/homebrew/lib/python3.11/site-packages (from torch) (2023.10.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/homebrew/lib/python3.11/site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/homebrew/lib/python3.11/site-packages (from sympy->torch) (1.3.0)\n",
      "Requirement already satisfied: requests in /opt/homebrew/lib/python3.11/site-packages (from torchvision->timm==0.5.4) (2.31.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/homebrew/lib/python3.11/site-packages (from torchvision->timm==0.5.4) (9.5.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/homebrew/lib/python3.11/site-packages (from requests->torchvision->timm==0.5.4) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/homebrew/lib/python3.11/site-packages (from requests->torchvision->timm==0.5.4) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/homebrew/lib/python3.11/site-packages (from requests->torchvision->timm==0.5.4) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/homebrew/lib/python3.11/site-packages (from requests->torchvision->timm==0.5.4) (2024.2.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.11 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install torch numpy timm==0.5.4 tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "383c6887",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/tsungchiehchen/Vision-Transformer.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78da3d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd ./Vision-Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e7507140",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import datetime\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "from timm.models import create_model\n",
    "\n",
    "from engine import train_one_epoch, train_one_epoch_distillation, evaluate\n",
    "from utils import get_training_dataloader, get_test_dataloader, get_imagenet_train_dataloader, get_imagenet_test_dataloader\n",
    "import models\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "234c6ce8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating model: vit_tiny_patch16_224\n",
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170498071/170498071 [00:16<00:00, 10475902.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
      "Files already downloaded and verified\n",
      "number of params: 5526346\n"
     ]
    }
   ],
   "source": [
    "MEAN = (0.5070751592371323, 0.48654887331495095, 0.4409178433670343)\n",
    "STD = (0.2673342858792401, 0.2564384629170883, 0.27615047132568404)\n",
    "CHECKPOINT_PATH = './checkpoint'\n",
    "MODEL_NAME = 'vit_tiny_patch16_224'\n",
    "num_classes = 10\n",
    "EPOCHS = 5\n",
    "LR = 0.0001\n",
    "WD = 0.0\n",
    "shots = 1000\n",
    "\n",
    "print(f\"Creating model: {MODEL_NAME}\")\n",
    "model = create_model(\n",
    "        MODEL_NAME,\n",
    "        pretrained=False,\n",
    "        num_classes=10,\n",
    "        img_size=224)\n",
    "device = 'cuda:0' # device = 'cpu'\n",
    "model = model.to(device)\n",
    "\n",
    "cifar10_training_loader = get_training_dataloader(\n",
    "    MEAN,\n",
    "    STD,\n",
    "    num_workers=2,\n",
    "    batch_size=16,\n",
    "    shuffle=True,\n",
    "    shots=shots\n",
    ")\n",
    "\n",
    "assert (shots*num_classes == len(cifar10_training_loader.dataset))\n",
    "\n",
    "cifar10_test_loader = get_test_dataloader(\n",
    "    MEAN,\n",
    "    STD,\n",
    "    num_workers=4,\n",
    "    batch_size=256,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR, weight_decay=WD)\n",
    "\n",
    "\n",
    "n_parameters = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print('number of params:', n_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f51e2794",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training for 5 epochs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStart training for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mEPOCHS\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m epochs\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, EPOCHS\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m----> 4\u001b[0m     train_stats \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcifar10_training_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m epoch \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m9\u001b[39m: \n\u001b[1;32m      8\u001b[0m         test_stats \u001b[38;5;241m=\u001b[39m evaluate(cifar10_test_loader, model, criterion, device)\n",
      "File \u001b[0;32m~/Documents/GitHub/Vision-Transformer/engine.py:42\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[0;34m(model, criterion, data_loader, optimizer, device, epoch)\u001b[0m\n\u001b[1;32m     39\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     40\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m---> 42\u001b[0m     \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msynchronize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m     metric_logger\u001b[38;5;241m.\u001b[39mupdate(loss\u001b[38;5;241m=\u001b[39mloss_value)\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAveraged stats:\u001b[39m\u001b[38;5;124m\"\u001b[39m, metric_logger)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/cuda/__init__.py:781\u001b[0m, in \u001b[0;36msynchronize\u001b[0;34m(device)\u001b[0m\n\u001b[1;32m    773\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msynchronize\u001b[39m(device: _device_t \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    774\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Waits for all kernels in all streams on a CUDA device to complete.\u001b[39;00m\n\u001b[1;32m    775\u001b[0m \n\u001b[1;32m    776\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    779\u001b[0m \u001b[38;5;124;03m            if :attr:`device` is ``None`` (default).\u001b[39;00m\n\u001b[1;32m    780\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 781\u001b[0m     \u001b[43m_lazy_init\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    782\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mdevice(device):\n\u001b[1;32m    783\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_cuda_synchronize()\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/cuda/__init__.py:289\u001b[0m, in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    284\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    285\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    286\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultiprocessing, you must use the \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspawn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m start method\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    287\u001b[0m     )\n\u001b[1;32m    288\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(torch\u001b[38;5;241m.\u001b[39m_C, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_cuda_getDeviceCount\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 289\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTorch not compiled with CUDA enabled\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    290\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _cudart \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    291\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[1;32m    292\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    293\u001b[0m     )\n",
      "\u001b[0;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "print(f\"Start training for {EPOCHS} epochs\")\n",
    "\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    train_stats = train_one_epoch(\n",
    "        model, criterion, cifar10_training_loader,\n",
    "        optimizer, device, epoch)\n",
    "    if epoch % 10 == 9: \n",
    "        test_stats = evaluate(cifar10_test_loader, model, criterion, device)\n",
    "        print(f\"Accuracy of the network on the {len(cifar10_test_loader)} test images: {test_stats['acc1']:.1f}%\")        \n",
    "        \n",
    "test_stats = evaluate(cifar10_test_loader, model, criterion, device)\n",
    "print(f\"Accuracy of the network on the {len(cifar10_test_loader)} test images: {test_stats['acc1']:.1f}%\")            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d48bd9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate througput \n",
    "start_time = time.time()\n",
    "test_stats = evaluate(cifar10_test_loader, model, criterion, device)\n",
    "end_time = time.time()\n",
    "num_samples = len(cifar10_test_loader.dataset)\n",
    "throughput = num_samples / (end_time - start_time)\n",
    "print(\"Throughput: {}\".format(throughput))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61fdc53f",
   "metadata": {},
   "source": [
    "# Q2 ImageNet Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee8523be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths and parameters\n",
    "batch_size = 256\n",
    "num_workers = 4\n",
    "\n",
    "# Load ImageNet data\n",
    "train_loader = get_imagenet_train_dataloader(batch_size=batch_size, num_workers=num_workers)\n",
    "val_loader = get_imagenet_test_dataloader(batch_size=batch_size, num_workers=num_workers)\n",
    "\n",
    "# Define model, criterion, optimizer, etc.\n",
    "MODEL_NAME = 'vit_base_patch16_224'\n",
    "num_classes = 1000  # ImageNet has 1000 classes\n",
    "EPOCHS = 5\n",
    "LR = 0.0001\n",
    "WD = 0.0\n",
    "\n",
    "print(f\"Creating model: {MODEL_NAME}\")\n",
    "model = create_model(\n",
    "        MODEL_NAME,\n",
    "        pretrained=True,\n",
    "        num_classes=num_classes,\n",
    "        img_size=224)\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "model = model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR, weight_decay=WD)\n",
    "\n",
    "# Training loop\n",
    "print(f\"Start training for {EPOCHS} epochs\")\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    train_stats = train_one_epoch(\n",
    "        model, criterion, train_loader,\n",
    "        optimizer, device, epoch)\n",
    "    if epoch % 1 == 0: \n",
    "        test_stats = evaluate(val_loader, model, criterion, device)\n",
    "        print(f\"Accuracy of the network on the validation images: {test_stats['acc1']:.1f}%\")        \n",
    "        \n",
    "test_stats = evaluate(val_loader, model, criterion, device)\n",
    "print(f\"Accuracy of the network on the validation images: {test_stats['acc1']:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e323d040",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate througput \n",
    "start_time = time.time()\n",
    "test_stats = evaluate(cifar10_test_loader, model, criterion, device)\n",
    "end_time = time.time()\n",
    "num_samples = len(cifar10_test_loader.dataset)\n",
    "throughput = num_samples / (end_time - start_time)\n",
    "print(\"Throughput: {}\".format(throughput))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a93beb48",
   "metadata": {},
   "source": [
    "# Q3 ViT model on a small device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb0666d",
   "metadata": {},
   "outputs": [],
   "source": [
    "MEAN = (0.5070751592371323, 0.48654887331495095, 0.4409178433670343)\n",
    "STD = (0.2673342858792401, 0.2564384629170883, 0.27615047132568404)\n",
    "CHECKPOINT_PATH = './checkpoint'\n",
    "MODEL_NAME = 'vit_tiny_patch16_224'\n",
    "num_classes = 10\n",
    "EPOCHS = 5\n",
    "LR = 0.0001\n",
    "WD = 0.0\n",
    "shots = 1000\n",
    "\n",
    "print(f\"Creating model: {MODEL_NAME}\")\n",
    "model = create_model(\n",
    "        MODEL_NAME,\n",
    "        pretrained=False,\n",
    "        num_classes=10,\n",
    "        img_size=224)\n",
    "device = 'cuda:0' # device = 'cpu'\n",
    "model = model.to(device)\n",
    "\n",
    "cifar10_training_loader = get_training_dataloader(\n",
    "    MEAN,\n",
    "    STD,\n",
    "    num_workers=2,\n",
    "    batch_size=16,\n",
    "    shuffle=True,\n",
    "    shots=shots\n",
    ")\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR, weight_decay=WD)\n",
    "\n",
    "\n",
    "n_parameters = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print('number of params:', n_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138b86de",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Start training for {EPOCHS} epochs\")\n",
    "\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    train_stats = train_one_epoch(\n",
    "        model, criterion, cifar10_training_loader,\n",
    "        optimizer, device, epoch)\n",
    "    if epoch % 10 == 9: \n",
    "        test_stats = evaluate(cifar10_test_loader, model, criterion, device)\n",
    "        print(f\"Accuracy of the network on the {len(cifar10_test_loader)} test images: {test_stats['acc1']:.1f}%\")        \n",
    "        \n",
    "test_stats = evaluate(cifar10_test_loader, model, criterion, device)\n",
    "print(f\"Accuracy of the network on the {len(cifar10_test_loader)} test images: {test_stats['acc1']:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199e33f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate througput \n",
    "start_time = time.time()\n",
    "test_stats = evaluate(cifar10_test_loader, model, criterion, device)\n",
    "end_time = time.time()\n",
    "num_samples = len(cifar10_test_loader.dataset)\n",
    "throughput = num_samples / (end_time - start_time)\n",
    "print(\"Throughput: {}\".format(throughput))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33816cf2",
   "metadata": {},
   "source": [
    "# Q4 Knowledge Distillation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caaf4f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Train the teacher\n",
    "\n",
    "MODEL_NAME = 'vit_base_patch16_224'\n",
    "num_classes = 10\n",
    "EPOCHS = 5\n",
    "LR = 0.0001\n",
    "WD = 0.0\n",
    "\n",
    "print(f\"Creating model: {MODEL_NAME}\")\n",
    "teacher = create_model(\n",
    "        MODEL_NAME,\n",
    "        pretrained=True,\n",
    "        num_classes=10,\n",
    "        img_size=224)\n",
    "device = 'cuda:0' # device = 'cpu'\n",
    "teacher = teacher.to(device)\n",
    "\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(teacher.parameters(), lr=LR, weight_decay=WD)\n",
    "\n",
    "\n",
    "n_parameters = sum(p.numel() for p in teacher.parameters() if p.requires_grad)\n",
    "print('number of params:', n_parameters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c392f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Start training for {EPOCHS} epochs\")\n",
    "\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    train_stats = train_one_epoch(\n",
    "        teacher, criterion, cifar10_training_loader,\n",
    "        optimizer, device, epoch)\n",
    "    if epoch % 10 == 9: \n",
    "        test_stats = evaluate(cifar10_test_loader, teacher, criterion, device)\n",
    "        print(f\"Accuracy of the network on the {len(cifar10_test_loader)} test images: {test_stats['acc1']:.1f}%\")        \n",
    "        \n",
    "test_stats = evaluate(cifar10_test_loader, teacher, criterion, device)\n",
    "print(f\"Accuracy of the network on the {len(cifar10_test_loader)} test images: {test_stats['acc1']:.1f}%\")       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab509fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save finetuned teacher model\n",
    "torch.save(teacher.state_dict(), './teacher.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69587aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "teacher = create_model(\n",
    "        'vit_base_patch16_224',\n",
    "        pretrained=True,\n",
    "        num_classes=10,\n",
    "        img_size=224)\n",
    "device = 'cuda:0' # device = 'cpu'\n",
    "teacher = teacher.to(device)\n",
    "teacher.load_state_dict(torch.load('./teacher.pth'))\n",
    "\n",
    "test_stats = evaluate(cifar10_test_loader, teacher, criterion, device)\n",
    "print(f\"Accuracy of the network on the {len(cifar10_test_loader)} test images: {test_stats['acc1']:.1f}%\")            \n",
    "\n",
    "# Train the student\n",
    "for p in teacher.parameters(): \n",
    "    p.requires_grad = False\n",
    "\n",
    "MODEL_NAME = 'vit_tiny_patch16_224'\n",
    "\n",
    "model = create_model(\n",
    "        MODEL_NAME,\n",
    "        pretrained=True,\n",
    "        num_classes=10,\n",
    "        img_size=224)\n",
    "device = 'cuda:0' # device = 'cpu'\n",
    "model = model.to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR, weight_decay=WD)\n",
    "\n",
    "\n",
    "n_parameters = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print('number of params:', n_parameters)\n",
    "\n",
    "\n",
    "print(f\"Start training for {EPOCHS} epochs\")\n",
    "\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    train_stats = train_one_epoch_distillation(\n",
    "        teacher, model, criterion, cifar10_training_loader,\n",
    "        optimizer, device, epoch, alpha=2.0, temp=1.0)\n",
    "    if epoch % 2 == 1: \n",
    "        test_stats = evaluate(cifar10_test_loader, model, criterion, device)\n",
    "        print(f\"Accuracy of the network on the {len(cifar10_test_loader)} test images: {test_stats['acc1']:.1f}%\")        \n",
    "        \n",
    "test_stats = evaluate(cifar10_test_loader, model, criterion, device)\n",
    "print(f\"Accuracy of the network on the {len(cifar10_test_loader)} test images: {test_stats['acc1']:.1f}%\")            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
