{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "c52b91c0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c52b91c0",
        "outputId": "7b2f3ccf-a6eb-4b0c-9854-42657459b90a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.3.0+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.25.2)\n",
            "Collecting timm==0.5.4\n",
            "  Downloading timm-0.5.4-py3-none-any.whl (431 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m431.5/431.5 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.4)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from timm==0.5.4) (0.18.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.3.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.40-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m73.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->timm==0.5.4) (9.4.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, timm\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.40 nvidia-nvtx-cu12-12.1.105 timm-0.5.4\n"
          ]
        }
      ],
      "source": [
        "! pip install torch numpy timm==0.5.4 tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "383c6887",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "383c6887",
        "outputId": "81daf812-4ee6-4e62-df1c-40259608bcfe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Vision-Transformer'...\n",
            "remote: Enumerating objects: 27, done.\u001b[K\n",
            "remote: Counting objects: 100% (27/27), done.\u001b[K\n",
            "remote: Compressing objects: 100% (20/20), done.\u001b[K\n",
            "remote: Total 27 (delta 10), reused 24 (delta 7), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (27/27), 24.42 KiB | 555.00 KiB/s, done.\n",
            "Resolving deltas: 100% (10/10), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/tsungchiehchen/Vision-Transformer.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "78da3d77",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "78da3d77",
        "outputId": "f9699300-2582-4d36-fd73-4c26cfdf3ab9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Vision-Transformer\n"
          ]
        }
      ],
      "source": [
        "%cd ./Vision-Transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "e7507140",
      "metadata": {
        "id": "e7507140"
      },
      "outputs": [],
      "source": [
        "import argparse\n",
        "import datetime\n",
        "import os\n",
        "import sys\n",
        "import numpy as np\n",
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.backends.cudnn as cudnn\n",
        "\n",
        "from timm.models import create_model\n",
        "\n",
        "from engine import train_one_epoch, train_one_epoch_distillation, evaluate\n",
        "from utils import get_training_dataloader, get_test_dataloader\n",
        "import models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "234c6ce8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "234c6ce8",
        "outputId": "e21eed76-b6b3-4265-d691-6078cf240e1c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Creating model: vit_base_patch16_224\n",
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 170498071/170498071 [00:02<00:00, 79445281.30it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n",
            "number of params: 86567656\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        }
      ],
      "source": [
        "MEAN = (0.5070751592371323, 0.48654887331495095, 0.4409178433670343)\n",
        "STD = (0.2673342858792401, 0.2564384629170883, 0.27615047132568404)\n",
        "CHECKPOINT_PATH = './checkpoint'\n",
        "MODEL_NAME = 'vit_base_patch16_224'\n",
        "num_classes = 10\n",
        "EPOCHS = 5\n",
        "LR = 0.0001\n",
        "WD = 0.0\n",
        "shots = 1000\n",
        "\n",
        "print(f\"Creating model: {MODEL_NAME}\")\n",
        "model = create_model(\n",
        "        MODEL_NAME,\n",
        "        pretrained=False,\n",
        "        num_classes=10,\n",
        "        img_size=224)\n",
        "device = 'cuda:0' # device = 'cpu'\n",
        "model = model.to(device)\n",
        "\n",
        "cifar10_training_loader = get_training_dataloader(\n",
        "    MEAN,\n",
        "    STD,\n",
        "    num_workers=2,\n",
        "    batch_size=16,\n",
        "    shuffle=True,\n",
        "    shots=shots\n",
        ")\n",
        "\n",
        "assert (shots*num_classes == len(cifar10_training_loader.dataset))\n",
        "\n",
        "cifar10_test_loader = get_test_dataloader(\n",
        "    MEAN,\n",
        "    STD,\n",
        "    num_workers=4,\n",
        "    batch_size=256,\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=LR, weight_decay=WD)\n",
        "\n",
        "\n",
        "n_parameters = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print('number of params:', n_parameters)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f51e2794",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f51e2794",
        "outputId": "1a6f5c56-b383-4aac-f881-6bb6793b75a4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Start training for 5 epochs\n",
            "Epoch: [1]  [  0/625]  eta: 0:28:48  loss: 7.2056 (7.2056)  time: 2.7654  data: 0.4869  max mem: 2096\n",
            "Epoch: [1]  [100/625]  eta: 0:01:44  loss: 2.2015 (2.4863)  time: 0.1713  data: 0.0057  max mem: 3065\n",
            "Epoch: [1]  [200/625]  eta: 0:01:19  loss: 2.1340 (2.3034)  time: 0.1701  data: 0.0055  max mem: 3065\n",
            "Epoch: [1]  [300/625]  eta: 0:00:59  loss: 2.0563 (2.2224)  time: 0.1778  data: 0.0084  max mem: 3065\n",
            "Epoch: [1]  [400/625]  eta: 0:00:40  loss: 2.1087 (2.1713)  time: 0.1774  data: 0.0065  max mem: 3065\n",
            "Epoch: [1]  [500/625]  eta: 0:00:22  loss: 1.8859 (2.1306)  time: 0.1793  data: 0.0054  max mem: 3065\n",
            "Epoch: [1]  [600/625]  eta: 0:00:04  loss: 1.9450 (2.1032)  time: 0.1898  data: 0.0100  max mem: 3065\n",
            "Epoch: [1]  [624/625]  eta: 0:00:00  loss: 2.0016 (2.0996)  time: 0.1834  data: 0.0051  max mem: 3065\n",
            "Epoch: [1] Total time: 0:01:53 (0.1819 s / it)\n",
            "Averaged stats: loss: 2.0016 (2.0996)\n",
            "Epoch: [2]  [  0/625]  eta: 0:03:33  loss: 2.1157 (2.1157)  time: 0.3417  data: 0.1587  max mem: 3065\n",
            "Epoch: [2]  [100/625]  eta: 0:01:40  loss: 1.8432 (1.9094)  time: 0.1923  data: 0.0085  max mem: 3065\n",
            "Epoch: [2]  [200/625]  eta: 0:01:20  loss: 1.8190 (1.8919)  time: 0.1830  data: 0.0053  max mem: 3065\n",
            "Epoch: [2]  [300/625]  eta: 0:01:01  loss: 1.8658 (1.8762)  time: 0.1825  data: 0.0054  max mem: 3065\n",
            "Epoch: [2]  [400/625]  eta: 0:00:42  loss: 1.7444 (1.8771)  time: 0.1865  data: 0.0079  max mem: 3065\n",
            "Epoch: [2]  [500/625]  eta: 0:00:23  loss: 1.8485 (1.8693)  time: 0.1840  data: 0.0053  max mem: 3065\n",
            "Epoch: [2]  [600/625]  eta: 0:00:04  loss: 1.7978 (1.8610)  time: 0.1920  data: 0.0105  max mem: 3065\n",
            "Epoch: [2]  [624/625]  eta: 0:00:00  loss: 1.8339 (1.8594)  time: 0.1836  data: 0.0055  max mem: 3065\n",
            "Epoch: [2] Total time: 0:01:56 (0.1868 s / it)\n",
            "Averaged stats: loss: 1.8339 (1.8594)\n",
            "Epoch: [3]  [  0/625]  eta: 0:03:24  loss: 1.9638 (1.9638)  time: 0.3265  data: 0.1391  max mem: 3065\n",
            "Epoch: [3]  [100/625]  eta: 0:01:37  loss: 1.7909 (1.7991)  time: 0.1826  data: 0.0055  max mem: 3065\n",
            "Epoch: [3]  [200/625]  eta: 0:01:18  loss: 1.6734 (1.7751)  time: 0.1843  data: 0.0066  max mem: 3065\n",
            "Epoch: [3]  [300/625]  eta: 0:01:00  loss: 1.6332 (1.7771)  time: 0.1824  data: 0.0054  max mem: 3065\n",
            "Epoch: [3]  [400/625]  eta: 0:00:41  loss: 1.8108 (1.8007)  time: 0.1913  data: 0.0105  max mem: 3065\n",
            "Epoch: [3]  [500/625]  eta: 0:00:23  loss: 1.8656 (1.7931)  time: 0.1840  data: 0.0054  max mem: 3065\n",
            "Epoch: [3]  [600/625]  eta: 0:00:04  loss: 1.7229 (1.7936)  time: 0.1863  data: 0.0079  max mem: 3065\n",
            "Epoch: [3]  [624/625]  eta: 0:00:00  loss: 1.6740 (1.7939)  time: 0.1855  data: 0.0065  max mem: 3065\n",
            "Epoch: [3] Total time: 0:01:56 (0.1858 s / it)\n",
            "Averaged stats: loss: 1.6740 (1.7939)\n",
            "Epoch: [4]  [  0/625]  eta: 0:03:36  loss: 1.2824 (1.2824)  time: 0.3457  data: 0.1420  max mem: 3065\n",
            "Epoch: [4]  [100/625]  eta: 0:01:38  loss: 1.7753 (1.7321)  time: 0.1835  data: 0.0059  max mem: 3065\n",
            "Epoch: [4]  [200/625]  eta: 0:01:19  loss: 1.6732 (1.7365)  time: 0.1914  data: 0.0103  max mem: 3065\n",
            "Epoch: [4]  [300/625]  eta: 0:01:00  loss: 1.6920 (1.7324)  time: 0.1826  data: 0.0054  max mem: 3065\n",
            "Epoch: [4]  [400/625]  eta: 0:00:41  loss: 1.6332 (1.7279)  time: 0.1845  data: 0.0071  max mem: 3065\n",
            "Epoch: [4]  [500/625]  eta: 0:00:23  loss: 1.7922 (1.7346)  time: 0.1839  data: 0.0061  max mem: 3065\n",
            "Epoch: [4]  [600/625]  eta: 0:00:04  loss: 1.6619 (1.7348)  time: 0.1832  data: 0.0060  max mem: 3065\n",
            "Epoch: [4]  [624/625]  eta: 0:00:00  loss: 1.7109 (1.7339)  time: 0.1882  data: 0.0091  max mem: 3065\n",
            "Epoch: [4] Total time: 0:01:55 (0.1855 s / it)\n",
            "Averaged stats: loss: 1.7109 (1.7339)\n",
            "Epoch: [5]  [  0/625]  eta: 0:03:26  loss: 1.4502 (1.4502)  time: 0.3312  data: 0.1415  max mem: 3065\n",
            "Epoch: [5]  [100/625]  eta: 0:01:37  loss: 1.5983 (1.6231)  time: 0.1826  data: 0.0054  max mem: 3065\n",
            "Epoch: [5]  [200/625]  eta: 0:01:19  loss: 1.7797 (1.6652)  time: 0.1847  data: 0.0058  max mem: 3065\n",
            "Epoch: [5]  [300/625]  eta: 0:01:00  loss: 1.7249 (1.6864)  time: 0.1837  data: 0.0058  max mem: 3065\n",
            "Epoch: [5]  [400/625]  eta: 0:00:41  loss: 1.6766 (1.6921)  time: 0.1832  data: 0.0054  max mem: 3065\n",
            "Epoch: [5]  [500/625]  eta: 0:00:23  loss: 1.7141 (1.6965)  time: 0.1891  data: 0.0090  max mem: 3065\n",
            "Epoch: [5]  [600/625]  eta: 0:00:04  loss: 1.5961 (1.6943)  time: 0.1829  data: 0.0058  max mem: 3065\n",
            "Epoch: [5]  [624/625]  eta: 0:00:00  loss: 1.6461 (1.6918)  time: 0.1839  data: 0.0059  max mem: 3065\n",
            "Epoch: [5] Total time: 0:01:55 (0.1854 s / it)\n",
            "Averaged stats: loss: 1.6461 (1.6918)\n",
            "Test:  [ 0/40]  eta: 0:03:07  loss: 1.6862 (1.6862)  acc1: 40.6250 (40.6250)  acc5: 85.1562 (85.1562)  time: 4.6876  data: 3.2015  max mem: 3158\n",
            "Test:  [20/40]  eta: 0:00:24  loss: 1.7490 (1.7590)  acc1: 36.3281 (35.9189)  acc5: 88.6719 (88.4301)  time: 1.0772  data: 0.0756  max mem: 3158\n",
            "Test:  [39/40]  eta: 0:00:01  loss: 1.7465 (1.7480)  acc1: 36.7188 (36.1300)  acc5: 88.6719 (88.0300)  time: 1.0213  data: 0.0639  max mem: 3158\n",
            "Test: Total time: 0:00:45 (1.1429 s / it)\n",
            "* Acc@1 36.130 Acc@5 88.030 loss 1.748\n",
            "Accuracy of the network on the 40 test images: 36.1%\n"
          ]
        }
      ],
      "source": [
        "print(f\"Start training for {EPOCHS} epochs\")\n",
        "\n",
        "for epoch in range(1, EPOCHS+1):\n",
        "    train_stats = train_one_epoch(\n",
        "        model, criterion, cifar10_training_loader,\n",
        "        optimizer, device, epoch)\n",
        "    if epoch % 10 == 9:\n",
        "        test_stats = evaluate(cifar10_test_loader, model, criterion, device)\n",
        "        print(f\"Accuracy of the network on the {len(cifar10_test_loader)} test images: {test_stats['acc1']:.1f}%\")\n",
        "\n",
        "test_stats = evaluate(cifar10_test_loader, model, criterion, device)\n",
        "print(f\"Accuracy of the network on the {len(cifar10_test_loader)} test images: {test_stats['acc1']:.1f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2d48bd9a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2d48bd9a",
        "outputId": "b4d85494-6205-403f-9aaf-db7e39fbc453"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test:  [ 0/40]  eta: 0:01:58  loss: 1.6862 (1.6862)  acc1: 40.6250 (40.6250)  acc5: 85.1562 (85.1562)  time: 2.9730  data: 1.8310  max mem: 3158\n",
            "Test:  [20/40]  eta: 0:00:23  loss: 1.7490 (1.7590)  acc1: 36.3281 (35.9189)  acc5: 88.6719 (88.4301)  time: 1.0917  data: 0.0786  max mem: 3158\n",
            "Test:  [39/40]  eta: 0:00:01  loss: 1.7465 (1.7480)  acc1: 36.7188 (36.1300)  acc5: 88.6719 (88.0300)  time: 1.0233  data: 0.0669  max mem: 3158\n",
            "Test: Total time: 0:00:44 (1.1050 s / it)\n",
            "* Acc@1 36.130 Acc@5 88.030 loss 1.748\n",
            "Throughput: 226.21906627248507\n"
          ]
        }
      ],
      "source": [
        "# Calculate througput\n",
        "start_time = time.time()\n",
        "test_stats = evaluate(cifar10_test_loader, model, criterion, device)\n",
        "end_time = time.time()\n",
        "num_samples = len(cifar10_test_loader.dataset)\n",
        "throughput = num_samples / (end_time - start_time)\n",
        "print(\"Throughput: {}\".format(throughput))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "61fdc53f",
      "metadata": {
        "id": "61fdc53f"
      },
      "source": [
        "# Q2 Fine-tuning Pretrained ViT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ee8523be",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ee8523be",
        "outputId": "f27399e5-7f2e-470b-b050-e8834ab51c10"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Creating model: vit_base_patch16_224\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading: \"https://dl.fbaipublicfiles.com/deit/deit_base_patch16_224-b5f2ef4d.pth\" to /root/.cache/torch/hub/checkpoints/deit_base_patch16_224-b5f2ef4d.pth\n",
            "100%|██████████| 330M/330M [00:05<00:00, 59.9MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "number of params: 85806346\n"
          ]
        }
      ],
      "source": [
        "MEAN = (0.5070751592371323, 0.48654887331495095, 0.4409178433670343)\n",
        "STD = (0.2673342858792401, 0.2564384629170883, 0.27615047132568404)\n",
        "CHECKPOINT_PATH = './checkpoint'\n",
        "MODEL_NAME = 'vit_base_patch16_224'\n",
        "num_classes = 10\n",
        "EPOCHS = 5\n",
        "LR = 0.0001\n",
        "WD = 0.0\n",
        "shots = 1000\n",
        "\n",
        "print(f\"Creating model: {MODEL_NAME}\")\n",
        "model = create_model(\n",
        "        MODEL_NAME,\n",
        "        pretrained=True,\n",
        "        num_classes=10,\n",
        "        img_size=224)\n",
        "device = 'cuda:0' # device = 'cpu'\n",
        "model = model.to(device)\n",
        "\n",
        "cifar10_training_loader = get_training_dataloader(\n",
        "    MEAN,\n",
        "    STD,\n",
        "    num_workers=2,\n",
        "    batch_size=16,\n",
        "    shuffle=True,\n",
        "    shots=shots\n",
        ")\n",
        "\n",
        "assert (shots*num_classes == len(cifar10_training_loader.dataset))\n",
        "\n",
        "cifar10_test_loader = get_test_dataloader(\n",
        "    MEAN,\n",
        "    STD,\n",
        "    num_workers=4,\n",
        "    batch_size=256,\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=LR, weight_decay=WD)\n",
        "\n",
        "\n",
        "n_parameters = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print('number of params:', n_parameters)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f881d0c0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f881d0c0",
        "outputId": "876be38a-34d6-4708-a6ec-32cf23fa72db"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Start training for 5 epochs\n",
            "Epoch: [1]  [  0/625]  eta: 0:04:01  loss: 2.3790 (2.3790)  time: 0.3858  data: 0.1525  max mem: 3158\n",
            "Epoch: [1]  [100/625]  eta: 0:01:38  loss: 2.0979 (2.2390)  time: 0.1886  data: 0.0055  max mem: 3158\n",
            "Epoch: [1]  [200/625]  eta: 0:01:20  loss: 2.0896 (2.1396)  time: 0.1938  data: 0.0096  max mem: 3158\n",
            "Epoch: [1]  [300/625]  eta: 0:01:01  loss: 1.8388 (2.0790)  time: 0.1859  data: 0.0053  max mem: 3158\n",
            "Epoch: [1]  [400/625]  eta: 0:00:42  loss: 1.7095 (2.0115)  time: 0.1875  data: 0.0071  max mem: 3158\n",
            "Epoch: [1]  [500/625]  eta: 0:00:23  loss: 1.5865 (1.9341)  time: 0.1861  data: 0.0057  max mem: 3158\n",
            "Epoch: [1]  [600/625]  eta: 0:00:04  loss: 1.4219 (1.8547)  time: 0.1886  data: 0.0058  max mem: 3158\n",
            "Epoch: [1]  [624/625]  eta: 0:00:00  loss: 1.4646 (1.8390)  time: 0.1900  data: 0.0065  max mem: 3158\n",
            "Epoch: [1] Total time: 0:01:58 (0.1890 s / it)\n",
            "Averaged stats: loss: 1.4646 (1.8390)\n",
            "Epoch: [2]  [  0/625]  eta: 0:03:39  loss: 1.2923 (1.2923)  time: 0.3512  data: 0.1692  max mem: 3158\n",
            "Epoch: [2]  [100/625]  eta: 0:01:40  loss: 1.1733 (1.2888)  time: 0.1884  data: 0.0053  max mem: 3158\n",
            "Epoch: [2]  [200/625]  eta: 0:01:21  loss: 1.0698 (1.2387)  time: 0.1939  data: 0.0105  max mem: 3158\n",
            "Epoch: [2]  [300/625]  eta: 0:01:01  loss: 0.9309 (1.1953)  time: 0.1869  data: 0.0053  max mem: 3158\n",
            "Epoch: [2]  [400/625]  eta: 0:00:42  loss: 1.0493 (1.1552)  time: 0.1945  data: 0.0101  max mem: 3158\n",
            "Epoch: [2]  [500/625]  eta: 0:00:23  loss: 0.9067 (1.1138)  time: 0.1880  data: 0.0051  max mem: 3158\n",
            "Epoch: [2]  [600/625]  eta: 0:00:04  loss: 0.9747 (1.0836)  time: 0.1916  data: 0.0073  max mem: 3158\n",
            "Epoch: [2]  [624/625]  eta: 0:00:00  loss: 0.9592 (1.0760)  time: 0.1878  data: 0.0054  max mem: 3158\n",
            "Epoch: [2] Total time: 0:01:59 (0.1904 s / it)\n",
            "Averaged stats: loss: 0.9592 (1.0760)\n",
            "Epoch: [3]  [  0/625]  eta: 0:03:41  loss: 0.7230 (0.7230)  time: 0.3548  data: 0.1660  max mem: 3158\n",
            "Epoch: [3]  [100/625]  eta: 0:01:39  loss: 0.7007 (0.7844)  time: 0.1872  data: 0.0054  max mem: 3158\n",
            "Epoch: [3]  [200/625]  eta: 0:01:20  loss: 0.7821 (0.7712)  time: 0.1894  data: 0.0068  max mem: 3158\n",
            "Epoch: [3]  [300/625]  eta: 0:01:01  loss: 0.7675 (0.7505)  time: 0.1877  data: 0.0057  max mem: 3158\n",
            "Epoch: [3]  [400/625]  eta: 0:00:42  loss: 0.5703 (0.7351)  time: 0.1893  data: 0.0064  max mem: 3158\n",
            "Epoch: [3]  [500/625]  eta: 0:00:23  loss: 0.5558 (0.7201)  time: 0.1869  data: 0.0054  max mem: 3158\n",
            "Epoch: [3]  [600/625]  eta: 0:00:04  loss: 0.5964 (0.7125)  time: 0.1928  data: 0.0092  max mem: 3158\n",
            "Epoch: [3]  [624/625]  eta: 0:00:00  loss: 0.6316 (0.7110)  time: 0.1875  data: 0.0052  max mem: 3158\n",
            "Epoch: [3] Total time: 0:01:58 (0.1896 s / it)\n",
            "Averaged stats: loss: 0.6316 (0.7110)\n",
            "Epoch: [4]  [  0/625]  eta: 0:03:48  loss: 0.6848 (0.6848)  time: 0.3655  data: 0.1727  max mem: 3158\n",
            "Epoch: [4]  [100/625]  eta: 0:01:39  loss: 0.4594 (0.5009)  time: 0.1866  data: 0.0056  max mem: 3158\n",
            "Epoch: [4]  [200/625]  eta: 0:01:20  loss: 0.5463 (0.5158)  time: 0.1900  data: 0.0077  max mem: 3158\n",
            "Epoch: [4]  [300/625]  eta: 0:01:01  loss: 0.4741 (0.5190)  time: 0.1865  data: 0.0060  max mem: 3158\n",
            "Epoch: [4]  [400/625]  eta: 0:00:42  loss: 0.4251 (0.5156)  time: 0.1938  data: 0.0111  max mem: 3158\n",
            "Epoch: [4]  [500/625]  eta: 0:00:23  loss: 0.4471 (0.5166)  time: 0.1867  data: 0.0056  max mem: 3158\n",
            "Epoch: [4]  [600/625]  eta: 0:00:04  loss: 0.5629 (0.5153)  time: 0.1912  data: 0.0091  max mem: 3158\n",
            "Epoch: [4]  [624/625]  eta: 0:00:00  loss: 0.5342 (0.5171)  time: 0.1866  data: 0.0052  max mem: 3158\n",
            "Epoch: [4] Total time: 0:01:58 (0.1890 s / it)\n",
            "Averaged stats: loss: 0.5342 (0.5171)\n",
            "Epoch: [5]  [  0/625]  eta: 0:03:51  loss: 0.4008 (0.4008)  time: 0.3707  data: 0.1841  max mem: 3158\n",
            "Epoch: [5]  [100/625]  eta: 0:01:40  loss: 0.2556 (0.3831)  time: 0.1874  data: 0.0062  max mem: 3158\n",
            "Epoch: [5]  [200/625]  eta: 0:01:20  loss: 0.3414 (0.3919)  time: 0.1874  data: 0.0067  max mem: 3158\n",
            "Epoch: [5]  [300/625]  eta: 0:01:01  loss: 0.4070 (0.3873)  time: 0.1866  data: 0.0053  max mem: 3158\n",
            "Epoch: [5]  [400/625]  eta: 0:00:42  loss: 0.3446 (0.3835)  time: 0.1916  data: 0.0100  max mem: 3158\n",
            "Epoch: [5]  [500/625]  eta: 0:00:23  loss: 0.3451 (0.3870)  time: 0.1853  data: 0.0052  max mem: 3158\n",
            "Epoch: [5]  [600/625]  eta: 0:00:04  loss: 0.4126 (0.3862)  time: 0.1933  data: 0.0105  max mem: 3158\n",
            "Epoch: [5]  [624/625]  eta: 0:00:00  loss: 0.3600 (0.3854)  time: 0.1858  data: 0.0054  max mem: 3158\n",
            "Epoch: [5] Total time: 0:01:57 (0.1888 s / it)\n",
            "Averaged stats: loss: 0.3600 (0.3854)\n",
            "Test:  [ 0/40]  eta: 0:02:18  loss: 0.6622 (0.6622)  acc1: 77.7344 (77.7344)  acc5: 98.4375 (98.4375)  time: 3.4508  data: 2.3775  max mem: 3158\n",
            "Test:  [20/40]  eta: 0:00:24  loss: 0.5921 (0.5938)  acc1: 80.0781 (80.3571)  acc5: 99.2188 (99.0699)  time: 1.0996  data: 0.0779  max mem: 3158\n",
            "Test:  [39/40]  eta: 0:00:01  loss: 0.5921 (0.5866)  acc1: 80.4688 (80.3500)  acc5: 99.2188 (99.1700)  time: 1.0202  data: 0.0550  max mem: 3158\n",
            "Test: Total time: 0:00:44 (1.1238 s / it)\n",
            "* Acc@1 80.350 Acc@5 99.170 loss 0.587\n",
            "Accuracy of the network on the 40 test images: 80.3%\n"
          ]
        }
      ],
      "source": [
        "print(f\"Start training for {EPOCHS} epochs\")\n",
        "\n",
        "for epoch in range(1, EPOCHS+1):\n",
        "    train_stats = train_one_epoch(\n",
        "        model, criterion, cifar10_training_loader,\n",
        "        optimizer, device, epoch)\n",
        "    if epoch % 10 == 9:\n",
        "        test_stats = evaluate(cifar10_test_loader, model, criterion, device)\n",
        "        print(f\"Accuracy of the network on the {len(cifar10_test_loader)} test images: {test_stats['acc1']:.1f}%\")\n",
        "\n",
        "test_stats = evaluate(cifar10_test_loader, model, criterion, device)\n",
        "print(f\"Accuracy of the network on the {len(cifar10_test_loader)} test images: {test_stats['acc1']:.1f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e323d040",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e323d040",
        "outputId": "b35ef800-3dcb-4413-9c2a-e2d01c9466dc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test:  [ 0/40]  eta: 0:02:29  loss: 0.6622 (0.6622)  acc1: 77.7344 (77.7344)  acc5: 98.4375 (98.4375)  time: 3.7399  data: 2.5347  max mem: 3158\n",
            "Test:  [20/40]  eta: 0:00:25  loss: 0.5921 (0.5938)  acc1: 80.0781 (80.3571)  acc5: 99.2188 (99.0699)  time: 1.1569  data: 0.1107  max mem: 3158\n",
            "Test:  [39/40]  eta: 0:00:01  loss: 0.5921 (0.5866)  acc1: 80.4688 (80.3500)  acc5: 99.2188 (99.1700)  time: 1.0263  data: 0.0535  max mem: 3364\n",
            "Test: Total time: 0:00:46 (1.1643 s / it)\n",
            "* Acc@1 80.350 Acc@5 99.170 loss 0.587\n",
            "Throughput: 214.69383078072903\n"
          ]
        }
      ],
      "source": [
        "# Calculate througput\n",
        "start_time = time.time()\n",
        "test_stats = evaluate(cifar10_test_loader, model, criterion, device)\n",
        "end_time = time.time()\n",
        "num_samples = len(cifar10_test_loader.dataset)\n",
        "throughput = num_samples / (end_time - start_time)\n",
        "print(\"Throughput: {}\".format(throughput))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a93beb48",
      "metadata": {
        "id": "a93beb48"
      },
      "source": [
        "# Q3 ViT model on a small device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "dcb0666d",
      "metadata": {
        "id": "dcb0666d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "735cada1-44d7-4d61-f603-a4fe26035c1e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating model: vit_tiny_patch16_224\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://dl.fbaipublicfiles.com/deit/deit_tiny_patch16_224-a1311bcf.pth\" to /root/.cache/torch/hub/checkpoints/deit_tiny_patch16_224-a1311bcf.pth\n",
            "100%|██████████| 21.9M/21.9M [00:00<00:00, 26.5MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170498071/170498071 [00:10<00:00, 16135777.33it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n",
            "number of params: 5526346\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        }
      ],
      "source": [
        "MEAN = (0.5070751592371323, 0.48654887331495095, 0.4409178433670343)\n",
        "STD = (0.2673342858792401, 0.2564384629170883, 0.27615047132568404)\n",
        "CHECKPOINT_PATH = './checkpoint'\n",
        "MODEL_NAME = 'vit_tiny_patch16_224'\n",
        "num_classes = 10\n",
        "EPOCHS = 5\n",
        "LR = 0.0001\n",
        "WD = 0.0\n",
        "shots = 1000\n",
        "\n",
        "print(f\"Creating model: {MODEL_NAME}\")\n",
        "model = create_model(\n",
        "        MODEL_NAME,\n",
        "        pretrained=True,\n",
        "        num_classes=10,\n",
        "        img_size=224)\n",
        "device = 'cuda:0' # device = 'cpu'\n",
        "model = model.to(device)\n",
        "\n",
        "cifar10_training_loader = get_training_dataloader(\n",
        "    MEAN,\n",
        "    STD,\n",
        "    num_workers=2,\n",
        "    batch_size=16,\n",
        "    shuffle=True,\n",
        "    shots=shots\n",
        ")\n",
        "\n",
        "assert (shots*num_classes == len(cifar10_training_loader.dataset))\n",
        "\n",
        "cifar10_test_loader = get_test_dataloader(\n",
        "    MEAN,\n",
        "    STD,\n",
        "    num_workers=4,\n",
        "    batch_size=256,\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=LR, weight_decay=WD)\n",
        "\n",
        "\n",
        "n_parameters = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print('number of params:', n_parameters)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "138b86de",
      "metadata": {
        "id": "138b86de",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ebd534b2-0c6a-48b4-f781-b0ea19828461"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start training for 5 epochs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py:456: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
            "  return F.conv2d(input, weight, bias, self.stride,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: [1]  [  0/625]  eta: 0:10:37  loss: 3.0119 (3.0119)  time: 1.0207  data: 0.1758  max mem: 463\n",
            "Epoch: [1]  [100/625]  eta: 0:00:39  loss: 2.0155 (2.1878)  time: 0.0863  data: 0.0097  max mem: 513\n",
            "Epoch: [1]  [200/625]  eta: 0:00:27  loss: 2.0032 (2.1210)  time: 0.0524  data: 0.0043  max mem: 513\n",
            "Epoch: [1]  [300/625]  eta: 0:00:19  loss: 2.0289 (2.0550)  time: 0.0705  data: 0.0077  max mem: 513\n",
            "Epoch: [1]  [400/625]  eta: 0:00:13  loss: 1.7647 (1.9978)  time: 0.0559  data: 0.0049  max mem: 513\n",
            "Epoch: [1]  [500/625]  eta: 0:00:07  loss: 1.5791 (1.9344)  time: 0.0530  data: 0.0050  max mem: 513\n",
            "Epoch: [1]  [600/625]  eta: 0:00:01  loss: 1.6066 (1.8826)  time: 0.0540  data: 0.0046  max mem: 513\n",
            "Epoch: [1]  [624/625]  eta: 0:00:00  loss: 1.6054 (1.8729)  time: 0.0509  data: 0.0043  max mem: 513\n",
            "Epoch: [1] Total time: 0:00:37 (0.0603 s / it)\n",
            "Averaged stats: loss: 1.6054 (1.8729)\n",
            "Epoch: [2]  [  0/625]  eta: 0:06:24  loss: 1.6375 (1.6375)  time: 0.6146  data: 0.3943  max mem: 513\n",
            "Epoch: [2]  [100/625]  eta: 0:00:31  loss: 1.4827 (1.4637)  time: 0.0632  data: 0.0078  max mem: 513\n",
            "Epoch: [2]  [200/625]  eta: 0:00:26  loss: 1.4724 (1.4457)  time: 0.0533  data: 0.0049  max mem: 513\n",
            "Epoch: [2]  [300/625]  eta: 0:00:19  loss: 1.1931 (1.3911)  time: 0.0556  data: 0.0049  max mem: 513\n",
            "Epoch: [2]  [400/625]  eta: 0:00:13  loss: 1.1899 (1.3565)  time: 0.0525  data: 0.0044  max mem: 513\n",
            "Epoch: [2]  [500/625]  eta: 0:00:07  loss: 1.3249 (1.3102)  time: 0.0551  data: 0.0048  max mem: 513\n",
            "Epoch: [2]  [600/625]  eta: 0:00:01  loss: 1.0825 (1.2884)  time: 0.0534  data: 0.0046  max mem: 513\n",
            "Epoch: [2]  [624/625]  eta: 0:00:00  loss: 0.9786 (1.2805)  time: 0.0499  data: 0.0050  max mem: 513\n",
            "Epoch: [2] Total time: 0:00:38 (0.0609 s / it)\n",
            "Averaged stats: loss: 0.9786 (1.2805)\n",
            "Epoch: [3]  [  0/625]  eta: 0:02:14  loss: 0.9453 (0.9453)  time: 0.2157  data: 0.1404  max mem: 513\n",
            "Epoch: [3]  [100/625]  eta: 0:00:29  loss: 0.8977 (0.9952)  time: 0.0542  data: 0.0047  max mem: 513\n",
            "Epoch: [3]  [200/625]  eta: 0:00:25  loss: 0.8573 (0.9822)  time: 0.0540  data: 0.0052  max mem: 513\n",
            "Epoch: [3]  [300/625]  eta: 0:00:18  loss: 1.0429 (0.9809)  time: 0.0524  data: 0.0046  max mem: 513\n",
            "Epoch: [3]  [400/625]  eta: 0:00:13  loss: 0.7869 (0.9642)  time: 0.0557  data: 0.0052  max mem: 513\n",
            "Epoch: [3]  [500/625]  eta: 0:00:07  loss: 1.0118 (0.9599)  time: 0.0520  data: 0.0046  max mem: 513\n",
            "Epoch: [3]  [600/625]  eta: 0:00:01  loss: 0.7411 (0.9464)  time: 0.0531  data: 0.0048  max mem: 513\n",
            "Epoch: [3]  [624/625]  eta: 0:00:00  loss: 0.8656 (0.9439)  time: 0.0515  data: 0.0045  max mem: 513\n",
            "Epoch: [3] Total time: 0:00:36 (0.0592 s / it)\n",
            "Averaged stats: loss: 0.8656 (0.9439)\n",
            "Epoch: [4]  [  0/625]  eta: 0:02:17  loss: 1.1614 (1.1614)  time: 0.2206  data: 0.1218  max mem: 513\n",
            "Epoch: [4]  [100/625]  eta: 0:00:29  loss: 0.7266 (0.7650)  time: 0.0534  data: 0.0046  max mem: 513\n",
            "Epoch: [4]  [200/625]  eta: 0:00:25  loss: 0.7209 (0.7736)  time: 0.0529  data: 0.0045  max mem: 513\n",
            "Epoch: [4]  [300/625]  eta: 0:00:18  loss: 0.7886 (0.7736)  time: 0.0560  data: 0.0048  max mem: 513\n",
            "Epoch: [4]  [400/625]  eta: 0:00:13  loss: 0.6786 (0.7620)  time: 0.0567  data: 0.0048  max mem: 513\n",
            "Epoch: [4]  [500/625]  eta: 0:00:07  loss: 0.6267 (0.7679)  time: 0.0540  data: 0.0048  max mem: 513\n",
            "Epoch: [4]  [600/625]  eta: 0:00:01  loss: 0.6121 (0.7652)  time: 0.0862  data: 0.0117  max mem: 513\n",
            "Epoch: [4]  [624/625]  eta: 0:00:00  loss: 0.5960 (0.7606)  time: 0.0501  data: 0.0043  max mem: 513\n",
            "Epoch: [4] Total time: 0:00:37 (0.0596 s / it)\n",
            "Averaged stats: loss: 0.5960 (0.7606)\n",
            "Epoch: [5]  [  0/625]  eta: 0:02:12  loss: 0.4276 (0.4276)  time: 0.2118  data: 0.1346  max mem: 513\n",
            "Epoch: [5]  [100/625]  eta: 0:00:29  loss: 0.5825 (0.6223)  time: 0.0542  data: 0.0047  max mem: 513\n",
            "Epoch: [5]  [200/625]  eta: 0:00:25  loss: 0.6348 (0.6353)  time: 0.0711  data: 0.0079  max mem: 513\n",
            "Epoch: [5]  [300/625]  eta: 0:00:18  loss: 0.5426 (0.6365)  time: 0.0539  data: 0.0047  max mem: 513\n",
            "Epoch: [5]  [400/625]  eta: 0:00:13  loss: 0.6739 (0.6369)  time: 0.0881  data: 0.0124  max mem: 513\n",
            "Epoch: [5]  [500/625]  eta: 0:00:07  loss: 0.6617 (0.6383)  time: 0.0543  data: 0.0047  max mem: 513\n",
            "Epoch: [5]  [600/625]  eta: 0:00:01  loss: 0.5862 (0.6372)  time: 0.0708  data: 0.0068  max mem: 513\n",
            "Epoch: [5]  [624/625]  eta: 0:00:00  loss: 0.5694 (0.6371)  time: 0.0806  data: 0.0094  max mem: 513\n",
            "Epoch: [5] Total time: 0:00:37 (0.0596 s / it)\n",
            "Averaged stats: loss: 0.5694 (0.6371)\n",
            "Test:  [ 0/40]  eta: 0:01:46  loss: 0.7713 (0.7713)  acc1: 71.8750 (71.8750)  acc5: 98.0469 (98.0469)  time: 2.6524  data: 1.8870  max mem: 632\n",
            "Test:  [20/40]  eta: 0:00:11  loss: 0.8422 (0.8150)  acc1: 71.0938 (71.9866)  acc5: 98.0469 (98.0655)  time: 0.4454  data: 0.1521  max mem: 760\n",
            "Test:  [39/40]  eta: 0:00:00  loss: 0.8209 (0.7990)  acc1: 72.2656 (72.4500)  acc5: 98.0469 (98.1400)  time: 0.4000  data: 0.1371  max mem: 760\n",
            "Test: Total time: 0:00:19 (0.4800 s / it)\n",
            "* Acc@1 72.450 Acc@5 98.140 loss 0.799\n",
            "Accuracy of the network on the 40 test images: 72.5%\n"
          ]
        }
      ],
      "source": [
        "print(f\"Start training for {EPOCHS} epochs\")\n",
        "\n",
        "for epoch in range(1, EPOCHS+1):\n",
        "    train_stats = train_one_epoch(\n",
        "        model, criterion, cifar10_training_loader,\n",
        "        optimizer, device, epoch)\n",
        "    if epoch % 10 == 9:\n",
        "        test_stats = evaluate(cifar10_test_loader, model, criterion, device)\n",
        "        print(f\"Accuracy of the network on the {len(cifar10_test_loader)} test images: {test_stats['acc1']:.1f}%\")\n",
        "\n",
        "test_stats = evaluate(cifar10_test_loader, model, criterion, device)\n",
        "print(f\"Accuracy of the network on the {len(cifar10_test_loader)} test images: {test_stats['acc1']:.1f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "199e33f5",
      "metadata": {
        "id": "199e33f5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d6aa085c-1a7c-4c36-96cb-05b0ec0bbe27"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test:  [ 0/40]  eta: 0:01:51  loss: 0.7713 (0.7713)  acc1: 71.8750 (71.8750)  acc5: 98.0469 (98.0469)  time: 2.7812  data: 2.4156  max mem: 760\n",
            "Test:  [20/40]  eta: 0:00:12  loss: 0.8422 (0.8150)  acc1: 71.0938 (71.9866)  acc5: 98.0469 (98.0655)  time: 0.5387  data: 0.2472  max mem: 760\n",
            "Test:  [39/40]  eta: 0:00:00  loss: 0.8209 (0.7990)  acc1: 72.2656 (72.4500)  acc5: 98.0469 (98.1400)  time: 0.4578  data: 0.1973  max mem: 760\n",
            "Test: Total time: 0:00:22 (0.5559 s / it)\n",
            "* Acc@1 72.450 Acc@5 98.140 loss 0.799\n",
            "Throughput: 449.6078784800346\n"
          ]
        }
      ],
      "source": [
        "# Calculate througput\n",
        "start_time = time.time()\n",
        "test_stats = evaluate(cifar10_test_loader, model, criterion, device)\n",
        "end_time = time.time()\n",
        "num_samples = len(cifar10_test_loader.dataset)\n",
        "throughput = num_samples / (end_time - start_time)\n",
        "print(\"Throughput: {}\".format(throughput))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "33816cf2",
      "metadata": {
        "id": "33816cf2"
      },
      "source": [
        "# Q4 Knowledge Distillation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "caaf4f8a",
      "metadata": {
        "id": "caaf4f8a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "81be5546-5b15-4a17-a9a9-02b36ff7c411"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating model: vit_base_patch16_224\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://dl.fbaipublicfiles.com/deit/deit_base_patch16_224-b5f2ef4d.pth\" to /root/.cache/torch/hub/checkpoints/deit_base_patch16_224-b5f2ef4d.pth\n",
            "100%|██████████| 330M/330M [00:03<00:00, 87.9MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of params: 85806346\n"
          ]
        }
      ],
      "source": [
        "# Step 1: Train the teacher\n",
        "MODEL_NAME = 'vit_base_patch16_224'\n",
        "num_classes = 10\n",
        "EPOCHS = 5\n",
        "LR = 0.0001\n",
        "WD = 0.0\n",
        "\n",
        "print(f\"Creating model: {MODEL_NAME}\")\n",
        "teacher = create_model(\n",
        "        MODEL_NAME,\n",
        "        pretrained=True,\n",
        "        num_classes=10,\n",
        "        img_size=224)\n",
        "device = 'cuda:0' # device = 'cpu'\n",
        "teacher = teacher.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(teacher.parameters(), lr=LR, weight_decay=WD)\n",
        "\n",
        "\n",
        "n_parameters = sum(p.numel() for p in teacher.parameters() if p.requires_grad)\n",
        "print('number of params:', n_parameters)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "6c392f94",
      "metadata": {
        "id": "6c392f94",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "03a1fdff-eb69-4ca9-e2c5-d4503bf6d901"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start training for 5 epochs\n",
            "Epoch: [1]  [  0/625]  eta: 0:04:16  loss: 2.4119 (2.4119)  time: 0.4107  data: 0.1498  max mem: 2139\n",
            "Epoch: [1]  [100/625]  eta: 0:01:34  loss: 1.7949 (2.1203)  time: 0.1803  data: 0.0077  max mem: 3092\n",
            "Epoch: [1]  [200/625]  eta: 0:01:17  loss: 1.3583 (1.8583)  time: 0.1773  data: 0.0046  max mem: 3092\n",
            "Epoch: [1]  [300/625]  eta: 0:00:58  loss: 1.1574 (1.6417)  time: 0.1801  data: 0.0054  max mem: 3092\n",
            "Epoch: [1]  [400/625]  eta: 0:00:40  loss: 1.0959 (1.4966)  time: 0.1803  data: 0.0051  max mem: 3092\n",
            "Epoch: [1]  [500/625]  eta: 0:00:22  loss: 0.8490 (1.3832)  time: 0.1796  data: 0.0047  max mem: 3092\n",
            "Epoch: [1]  [600/625]  eta: 0:00:04  loss: 0.7958 (1.2901)  time: 0.1836  data: 0.0076  max mem: 3092\n",
            "Epoch: [1]  [624/625]  eta: 0:00:00  loss: 0.7124 (1.2705)  time: 0.1796  data: 0.0043  max mem: 3092\n",
            "Epoch: [1] Total time: 0:01:53 (0.1815 s / it)\n",
            "Averaged stats: loss: 0.7124 (1.2705)\n",
            "Epoch: [2]  [  0/625]  eta: 0:03:29  loss: 0.6773 (0.6773)  time: 0.3359  data: 0.1482  max mem: 3092\n",
            "Epoch: [2]  [100/625]  eta: 0:01:36  loss: 0.6809 (0.7756)  time: 0.1856  data: 0.0065  max mem: 3092\n",
            "Epoch: [2]  [200/625]  eta: 0:01:18  loss: 0.5967 (0.7097)  time: 0.1820  data: 0.0048  max mem: 3092\n",
            "Epoch: [2]  [300/625]  eta: 0:00:59  loss: 0.6194 (0.6975)  time: 0.1859  data: 0.0073  max mem: 3092\n",
            "Epoch: [2]  [400/625]  eta: 0:00:41  loss: 0.6140 (0.6776)  time: 0.1826  data: 0.0053  max mem: 3092\n",
            "Epoch: [2]  [500/625]  eta: 0:00:22  loss: 0.5068 (0.6602)  time: 0.1827  data: 0.0050  max mem: 3092\n",
            "Epoch: [2]  [600/625]  eta: 0:00:04  loss: 0.4939 (0.6490)  time: 0.1832  data: 0.0058  max mem: 3092\n",
            "Epoch: [2]  [624/625]  eta: 0:00:00  loss: 0.4432 (0.6441)  time: 0.1837  data: 0.0060  max mem: 3092\n",
            "Epoch: [2] Total time: 0:01:54 (0.1840 s / it)\n",
            "Averaged stats: loss: 0.4432 (0.6441)\n",
            "Epoch: [3]  [  0/625]  eta: 0:04:17  loss: 1.1530 (1.1530)  time: 0.4122  data: 0.2364  max mem: 3092\n",
            "Epoch: [3]  [100/625]  eta: 0:01:38  loss: 0.4603 (0.5130)  time: 0.1881  data: 0.0090  max mem: 3092\n",
            "Epoch: [3]  [200/625]  eta: 0:01:18  loss: 0.4087 (0.4899)  time: 0.1823  data: 0.0049  max mem: 3092\n",
            "Epoch: [3]  [300/625]  eta: 0:01:00  loss: 0.4312 (0.4877)  time: 0.1866  data: 0.0083  max mem: 3092\n",
            "Epoch: [3]  [400/625]  eta: 0:00:41  loss: 0.3823 (0.4707)  time: 0.1824  data: 0.0049  max mem: 3092\n",
            "Epoch: [3]  [500/625]  eta: 0:00:23  loss: 0.4056 (0.4686)  time: 0.1847  data: 0.0065  max mem: 3092\n",
            "Epoch: [3]  [600/625]  eta: 0:00:04  loss: 0.4762 (0.4696)  time: 0.1824  data: 0.0050  max mem: 3092\n",
            "Epoch: [3]  [624/625]  eta: 0:00:00  loss: 0.3168 (0.4665)  time: 0.1821  data: 0.0046  max mem: 3092\n",
            "Epoch: [3] Total time: 0:01:55 (0.1845 s / it)\n",
            "Averaged stats: loss: 0.3168 (0.4665)\n",
            "Epoch: [4]  [  0/625]  eta: 0:03:29  loss: 0.5153 (0.5153)  time: 0.3353  data: 0.1532  max mem: 3092\n",
            "Epoch: [4]  [100/625]  eta: 0:01:37  loss: 0.2122 (0.3050)  time: 0.1829  data: 0.0056  max mem: 3092\n",
            "Epoch: [4]  [200/625]  eta: 0:01:18  loss: 0.2298 (0.3157)  time: 0.1817  data: 0.0046  max mem: 3092\n",
            "Epoch: [4]  [300/625]  eta: 0:01:00  loss: 0.3019 (0.3470)  time: 0.1873  data: 0.0092  max mem: 3092\n",
            "Epoch: [4]  [400/625]  eta: 0:00:41  loss: 0.3488 (0.3531)  time: 0.1820  data: 0.0050  max mem: 3092\n",
            "Epoch: [4]  [500/625]  eta: 0:00:23  loss: 0.2522 (0.3491)  time: 0.1870  data: 0.0080  max mem: 3092\n",
            "Epoch: [4]  [600/625]  eta: 0:00:04  loss: 0.2909 (0.3497)  time: 0.1825  data: 0.0051  max mem: 3092\n",
            "Epoch: [4]  [624/625]  eta: 0:00:00  loss: 0.3362 (0.3528)  time: 0.1827  data: 0.0048  max mem: 3092\n",
            "Epoch: [4] Total time: 0:01:55 (0.1844 s / it)\n",
            "Averaged stats: loss: 0.3362 (0.3528)\n",
            "Epoch: [5]  [  0/625]  eta: 0:04:15  loss: 0.2616 (0.2616)  time: 0.4083  data: 0.2294  max mem: 3092\n",
            "Epoch: [5]  [100/625]  eta: 0:01:38  loss: 0.2322 (0.2369)  time: 0.1831  data: 0.0057  max mem: 3092\n",
            "Epoch: [5]  [200/625]  eta: 0:01:19  loss: 0.1546 (0.2451)  time: 0.1824  data: 0.0054  max mem: 3092\n",
            "Epoch: [5]  [300/625]  eta: 0:01:00  loss: 0.2714 (0.2470)  time: 0.1842  data: 0.0067  max mem: 3092\n",
            "Epoch: [5]  [400/625]  eta: 0:00:41  loss: 0.2926 (0.2661)  time: 0.1825  data: 0.0046  max mem: 3092\n",
            "Epoch: [5]  [500/625]  eta: 0:00:23  loss: 0.1984 (0.2616)  time: 0.1877  data: 0.0079  max mem: 3092\n",
            "Epoch: [5]  [600/625]  eta: 0:00:04  loss: 0.3531 (0.2716)  time: 0.1824  data: 0.0049  max mem: 3092\n",
            "Epoch: [5]  [624/625]  eta: 0:00:00  loss: 0.2832 (0.2734)  time: 0.1843  data: 0.0057  max mem: 3092\n",
            "Epoch: [5] Total time: 0:01:55 (0.1849 s / it)\n",
            "Averaged stats: loss: 0.2832 (0.2734)\n",
            "Test:  [ 0/40]  eta: 0:02:27  loss: 0.5829 (0.5829)  acc1: 82.4219 (82.4219)  acc5: 98.4375 (98.4375)  time: 3.6825  data: 2.5635  max mem: 3175\n",
            "Test:  [20/40]  eta: 0:00:23  loss: 0.5100 (0.5287)  acc1: 83.2031 (83.3519)  acc5: 99.2188 (99.2746)  time: 1.0465  data: 0.0642  max mem: 3175\n",
            "Test:  [39/40]  eta: 0:00:01  loss: 0.5019 (0.5256)  acc1: 83.2031 (83.2100)  acc5: 99.2188 (99.2400)  time: 0.9711  data: 0.0504  max mem: 3175\n",
            "Test: Total time: 0:00:43 (1.0789 s / it)\n",
            "* Acc@1 83.210 Acc@5 99.240 loss 0.526\n",
            "Accuracy of the network on the 40 test images: 83.2%\n"
          ]
        }
      ],
      "source": [
        "print(f\"Start training for {EPOCHS} epochs\")\n",
        "\n",
        "for epoch in range(1, EPOCHS+1):\n",
        "    train_stats = train_one_epoch(\n",
        "        teacher, criterion, cifar10_training_loader,\n",
        "        optimizer, device, epoch)\n",
        "    if epoch % 10 == 9:\n",
        "        test_stats = evaluate(cifar10_test_loader, teacher, criterion, device)\n",
        "        print(f\"Accuracy of the network on the {len(cifar10_test_loader)} test images: {test_stats['acc1']:.1f}%\")\n",
        "\n",
        "test_stats = evaluate(cifar10_test_loader, teacher, criterion, device)\n",
        "print(f\"Accuracy of the network on the {len(cifar10_test_loader)} test images: {test_stats['acc1']:.1f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "9ab509fd",
      "metadata": {
        "id": "9ab509fd"
      },
      "outputs": [],
      "source": [
        "# save finetuned teacher model\n",
        "torch.save(teacher.state_dict(), './teacher.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "69587aa8",
      "metadata": {
        "id": "69587aa8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "61ac0311-98c6-4aa3-e731-f5107a6dba56"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test:  [ 0/40]  eta: 0:01:53  loss: 0.5829 (0.5829)  acc1: 82.4219 (82.4219)  acc5: 98.4375 (98.4375)  time: 2.8268  data: 1.7344  max mem: 3510\n",
            "Test:  [20/40]  eta: 0:00:22  loss: 0.5100 (0.5287)  acc1: 83.2031 (83.3519)  acc5: 99.2188 (99.2746)  time: 1.0361  data: 0.0759  max mem: 3510\n",
            "Test:  [39/40]  eta: 0:00:01  loss: 0.5019 (0.5256)  acc1: 83.2031 (83.2100)  acc5: 99.2188 (99.2400)  time: 0.9519  data: 0.0510  max mem: 3510\n",
            "Test: Total time: 0:00:41 (1.0429 s / it)\n",
            "* Acc@1 83.210 Acc@5 99.240 loss 0.526\n",
            "Accuracy of the network on the 40 test images: 83.2%\n",
            "number of params: 5526346\n",
            "Start training for 5 epochs\n",
            "Epoch: [1]  [  0/625]  eta: 0:03:26  loss: 2.1664 (2.1664)  time: 0.3309  data: 0.1566  max mem: 3510\n",
            "Epoch: [1]  [100/625]  eta: 0:00:56  loss: 2.1607 (2.2988)  time: 0.0994  data: 0.0045  max mem: 3510\n",
            "Epoch: [1]  [200/625]  eta: 0:00:45  loss: 2.0223 (2.1863)  time: 0.1005  data: 0.0047  max mem: 3510\n",
            "Epoch: [1]  [300/625]  eta: 0:00:34  loss: 1.8026 (2.1052)  time: 0.1070  data: 0.0072  max mem: 3510\n",
            "Epoch: [1]  [400/625]  eta: 0:00:23  loss: 1.6921 (2.0323)  time: 0.1124  data: 0.0079  max mem: 3510\n",
            "Epoch: [1]  [500/625]  eta: 0:00:13  loss: 1.5264 (1.9682)  time: 0.1015  data: 0.0050  max mem: 3510\n",
            "Epoch: [1]  [600/625]  eta: 0:00:02  loss: 1.4619 (1.8952)  time: 0.0995  data: 0.0047  max mem: 3510\n",
            "Epoch: [1]  [624/625]  eta: 0:00:00  loss: 1.4132 (1.8757)  time: 0.0991  data: 0.0044  max mem: 3510\n",
            "Epoch: [1] Total time: 0:01:05 (0.1043 s / it)\n",
            "Averaged stats: loss: 1.4132 (1.8757)\n",
            "Test:  [ 0/40]  eta: 0:01:59  loss: 1.6949 (1.6949)  acc1: 41.0156 (41.0156)  acc5: 90.6250 (90.6250)  time: 2.9816  data: 2.5564  max mem: 3510\n",
            "Test:  [20/40]  eta: 0:00:11  loss: 1.5689 (1.5706)  acc1: 41.4062 (41.7597)  acc5: 92.5781 (92.6525)  time: 0.4631  data: 0.1770  max mem: 3510\n",
            "Test:  [39/40]  eta: 0:00:00  loss: 1.6088 (1.5740)  acc1: 41.0156 (41.7800)  acc5: 91.4062 (92.1600)  time: 0.4762  data: 0.1848  max mem: 3510\n",
            "Test: Total time: 0:00:21 (0.5320 s / it)\n",
            "* Acc@1 41.780 Acc@5 92.160 loss 1.574\n",
            "Accuracy of the network on the 40 test images: 41.8%\n",
            "Epoch: [2]  [  0/625]  eta: 0:03:04  loss: 1.6499 (1.6499)  time: 0.2953  data: 0.1580  max mem: 3510\n",
            "Epoch: [2]  [100/625]  eta: 0:00:55  loss: 1.2707 (1.3323)  time: 0.1168  data: 0.0101  max mem: 3510\n",
            "Epoch: [2]  [200/625]  eta: 0:00:44  loss: 1.2257 (1.3265)  time: 0.1049  data: 0.0067  max mem: 3510\n",
            "Epoch: [2]  [300/625]  eta: 0:00:33  loss: 1.0897 (1.3004)  time: 0.1005  data: 0.0050  max mem: 3510\n",
            "Epoch: [2]  [400/625]  eta: 0:00:23  loss: 1.0588 (1.2757)  time: 0.0997  data: 0.0053  max mem: 3510\n",
            "Epoch: [2]  [500/625]  eta: 0:00:12  loss: 1.0707 (1.2451)  time: 0.0999  data: 0.0047  max mem: 3510\n",
            "Epoch: [2]  [600/625]  eta: 0:00:02  loss: 1.1845 (1.2265)  time: 0.1093  data: 0.0072  max mem: 3510\n",
            "Epoch: [2]  [624/625]  eta: 0:00:00  loss: 0.8988 (1.2196)  time: 0.0994  data: 0.0045  max mem: 3510\n",
            "Epoch: [2] Total time: 0:01:05 (0.1040 s / it)\n",
            "Averaged stats: loss: 0.8988 (1.2196)\n",
            "Epoch: [3]  [  0/625]  eta: 0:02:50  loss: 1.2466 (1.2466)  time: 0.2736  data: 0.1542  max mem: 3510\n",
            "Epoch: [3]  [100/625]  eta: 0:00:55  loss: 0.9560 (0.9048)  time: 0.1019  data: 0.0051  max mem: 3510\n",
            "Epoch: [3]  [200/625]  eta: 0:00:44  loss: 0.7998 (0.9111)  time: 0.1154  data: 0.0090  max mem: 3510\n",
            "Epoch: [3]  [300/625]  eta: 0:00:33  loss: 0.8999 (0.9047)  time: 0.1008  data: 0.0050  max mem: 3510\n",
            "Epoch: [3]  [400/625]  eta: 0:00:23  loss: 0.7785 (0.9059)  time: 0.1003  data: 0.0045  max mem: 3510\n",
            "Epoch: [3]  [500/625]  eta: 0:00:13  loss: 0.8144 (0.8984)  time: 0.1003  data: 0.0046  max mem: 3510\n",
            "Epoch: [3]  [600/625]  eta: 0:00:02  loss: 0.7307 (0.8951)  time: 0.1011  data: 0.0047  max mem: 3510\n",
            "Epoch: [3]  [624/625]  eta: 0:00:00  loss: 0.7953 (0.8927)  time: 0.0992  data: 0.0049  max mem: 3510\n",
            "Epoch: [3] Total time: 0:01:05 (0.1043 s / it)\n",
            "Averaged stats: loss: 0.7953 (0.8927)\n",
            "Test:  [ 0/40]  eta: 0:01:33  loss: 0.9653 (0.9653)  acc1: 63.6719 (63.6719)  acc5: 96.0938 (96.0938)  time: 2.3488  data: 2.0361  max mem: 3510\n",
            "Test:  [20/40]  eta: 0:00:11  loss: 0.8553 (0.8632)  acc1: 68.7500 (68.7128)  acc5: 98.0469 (97.8795)  time: 0.4846  data: 0.1867  max mem: 3510\n",
            "Test:  [39/40]  eta: 0:00:00  loss: 0.8736 (0.8630)  acc1: 68.7500 (68.7100)  acc5: 98.0469 (97.9700)  time: 0.3449  data: 0.1049  max mem: 3510\n",
            "Test: Total time: 0:00:18 (0.4657 s / it)\n",
            "* Acc@1 68.710 Acc@5 97.970 loss 0.863\n",
            "Accuracy of the network on the 40 test images: 68.7%\n",
            "Epoch: [4]  [  0/625]  eta: 0:03:18  loss: 0.7121 (0.7121)  time: 0.3173  data: 0.1633  max mem: 3510\n",
            "Epoch: [4]  [100/625]  eta: 0:00:55  loss: 0.6689 (0.7242)  time: 0.1000  data: 0.0048  max mem: 3510\n",
            "Epoch: [4]  [200/625]  eta: 0:00:44  loss: 0.8482 (0.7393)  time: 0.0995  data: 0.0048  max mem: 3510\n",
            "Epoch: [4]  [300/625]  eta: 0:00:34  loss: 0.5952 (0.7306)  time: 0.0994  data: 0.0048  max mem: 3510\n",
            "Epoch: [4]  [400/625]  eta: 0:00:23  loss: 0.6722 (0.7331)  time: 0.1159  data: 0.0087  max mem: 3510\n",
            "Epoch: [4]  [500/625]  eta: 0:00:12  loss: 0.7018 (0.7296)  time: 0.1020  data: 0.0051  max mem: 3510\n",
            "Epoch: [4]  [600/625]  eta: 0:00:02  loss: 0.6942 (0.7354)  time: 0.0996  data: 0.0046  max mem: 3510\n",
            "Epoch: [4]  [624/625]  eta: 0:00:00  loss: 0.6445 (0.7365)  time: 0.1002  data: 0.0048  max mem: 3510\n",
            "Epoch: [4] Total time: 0:01:04 (0.1039 s / it)\n",
            "Averaged stats: loss: 0.6445 (0.7365)\n",
            "Epoch: [5]  [  0/625]  eta: 0:04:10  loss: 0.5810 (0.5810)  time: 0.4015  data: 0.2553  max mem: 3510\n",
            "Epoch: [5]  [100/625]  eta: 0:00:56  loss: 0.5223 (0.6216)  time: 0.1002  data: 0.0049  max mem: 3510\n",
            "Epoch: [5]  [200/625]  eta: 0:00:44  loss: 0.5573 (0.6019)  time: 0.0998  data: 0.0049  max mem: 3510\n",
            "Epoch: [5]  [300/625]  eta: 0:00:34  loss: 0.3728 (0.6041)  time: 0.1004  data: 0.0054  max mem: 3510\n",
            "Epoch: [5]  [400/625]  eta: 0:00:23  loss: 0.5515 (0.6099)  time: 0.1004  data: 0.0055  max mem: 3510\n",
            "Epoch: [5]  [500/625]  eta: 0:00:13  loss: 0.7012 (0.6196)  time: 0.1208  data: 0.0097  max mem: 3510\n",
            "Epoch: [5]  [600/625]  eta: 0:00:02  loss: 0.6291 (0.6206)  time: 0.0997  data: 0.0045  max mem: 3510\n",
            "Epoch: [5]  [624/625]  eta: 0:00:00  loss: 0.5327 (0.6178)  time: 0.1169  data: 0.0098  max mem: 3510\n",
            "Epoch: [5] Total time: 0:01:06 (0.1057 s / it)\n",
            "Averaged stats: loss: 0.5327 (0.6178)\n",
            "Test:  [ 0/40]  eta: 0:01:33  loss: 0.9178 (0.9178)  acc1: 67.9688 (67.9688)  acc5: 96.4844 (96.4844)  time: 2.3362  data: 1.9731  max mem: 3510\n",
            "Test:  [20/40]  eta: 0:00:10  loss: 0.7614 (0.7781)  acc1: 74.6094 (73.8653)  acc5: 98.8281 (98.3445)  time: 0.4453  data: 0.1484  max mem: 3510\n",
            "Test:  [39/40]  eta: 0:00:00  loss: 0.7665 (0.7688)  acc1: 73.0469 (73.6000)  acc5: 98.8281 (98.4200)  time: 0.3918  data: 0.1298  max mem: 3510\n",
            "Test: Total time: 0:00:18 (0.4655 s / it)\n",
            "* Acc@1 73.600 Acc@5 98.420 loss 0.769\n",
            "Accuracy of the network on the 40 test images: 73.6%\n",
            "Test:  [ 0/40]  eta: 0:01:27  loss: 0.9178 (0.9178)  acc1: 67.9688 (67.9688)  acc5: 96.4844 (96.4844)  time: 2.1853  data: 1.8568  max mem: 3510\n",
            "Test:  [20/40]  eta: 0:00:10  loss: 0.7614 (0.7781)  acc1: 74.6094 (73.8653)  acc5: 98.8281 (98.3445)  time: 0.4659  data: 0.1680  max mem: 3510\n",
            "Test:  [39/40]  eta: 0:00:00  loss: 0.7665 (0.7688)  acc1: 73.0469 (73.6000)  acc5: 98.8281 (98.4200)  time: 0.3537  data: 0.1073  max mem: 3510\n",
            "Test: Total time: 0:00:18 (0.4578 s / it)\n",
            "* Acc@1 73.600 Acc@5 98.420 loss 0.769\n",
            "Accuracy of the network on the 40 test images: 73.6%\n"
          ]
        }
      ],
      "source": [
        "teacher = create_model(\n",
        "        'vit_base_patch16_224',\n",
        "        pretrained=True,\n",
        "        num_classes=10,\n",
        "        img_size=224)\n",
        "device = 'cuda:0' # device = 'cpu'\n",
        "teacher = teacher.to(device)\n",
        "teacher.load_state_dict(torch.load('./teacher.pth'))\n",
        "\n",
        "test_stats = evaluate(cifar10_test_loader, teacher, criterion, device)\n",
        "print(f\"Accuracy of the network on the {len(cifar10_test_loader)} test images: {test_stats['acc1']:.1f}%\")\n",
        "\n",
        "# Train the student\n",
        "for p in teacher.parameters():\n",
        "    p.requires_grad = False\n",
        "\n",
        "MODEL_NAME = 'vit_tiny_patch16_224'\n",
        "\n",
        "model = create_model(\n",
        "        MODEL_NAME,\n",
        "        pretrained=True,\n",
        "        num_classes=10,\n",
        "        img_size=224)\n",
        "device = 'cuda:0' # device = 'cpu'\n",
        "model = model.to(device)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=LR, weight_decay=WD)\n",
        "\n",
        "\n",
        "n_parameters = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print('number of params:', n_parameters)\n",
        "\n",
        "\n",
        "print(f\"Start training for {EPOCHS} epochs\")\n",
        "\n",
        "for epoch in range(1, EPOCHS+1):\n",
        "    train_stats = train_one_epoch_distillation(\n",
        "        teacher, model, criterion, cifar10_training_loader,\n",
        "        optimizer, device, epoch, alpha=1.0, temp=1.0)\n",
        "    if epoch % 2 == 1:\n",
        "        test_stats = evaluate(cifar10_test_loader, model, criterion, device)\n",
        "        print(f\"Accuracy of the network on the {len(cifar10_test_loader)} test images: {test_stats['acc1']:.1f}%\")\n",
        "\n",
        "test_stats = evaluate(cifar10_test_loader, model, criterion, device)\n",
        "print(f\"Accuracy of the network on the {len(cifar10_test_loader)} test images: {test_stats['acc1']:.1f}%\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}