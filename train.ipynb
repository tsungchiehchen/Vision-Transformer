{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c52b91c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /opt/homebrew/lib/python3.11/site-packages (2.1.1)\n",
      "Requirement already satisfied: numpy in /opt/homebrew/lib/python3.11/site-packages (1.24.3)\n",
      "Requirement already satisfied: timm==0.5.4 in /opt/homebrew/lib/python3.11/site-packages (0.5.4)\n",
      "Requirement already satisfied: tqdm in /opt/homebrew/lib/python3.11/site-packages (4.66.2)\n",
      "Requirement already satisfied: torchvision in /opt/homebrew/lib/python3.11/site-packages (from timm==0.5.4) (0.16.1)\n",
      "Requirement already satisfied: filelock in /opt/homebrew/lib/python3.11/site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in /opt/homebrew/lib/python3.11/site-packages (from torch) (4.8.0)\n",
      "Requirement already satisfied: sympy in /opt/homebrew/lib/python3.11/site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/homebrew/lib/python3.11/site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in /opt/homebrew/lib/python3.11/site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /opt/homebrew/lib/python3.11/site-packages (from torch) (2023.10.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/homebrew/lib/python3.11/site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/homebrew/lib/python3.11/site-packages (from sympy->torch) (1.3.0)\n",
      "Requirement already satisfied: requests in /opt/homebrew/lib/python3.11/site-packages (from torchvision->timm==0.5.4) (2.31.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/homebrew/lib/python3.11/site-packages (from torchvision->timm==0.5.4) (9.5.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/homebrew/lib/python3.11/site-packages (from requests->torchvision->timm==0.5.4) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/homebrew/lib/python3.11/site-packages (from requests->torchvision->timm==0.5.4) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/homebrew/lib/python3.11/site-packages (from requests->torchvision->timm==0.5.4) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/homebrew/lib/python3.11/site-packages (from requests->torchvision->timm==0.5.4) (2024.2.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.11 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install torch numpy timm==0.5.4 tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "383c6887",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/tsungchiehchen/Vision-Transformer.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78da3d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd ./Vision-Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e7507140",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import datetime\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "from timm.models import create_model\n",
    "\n",
    "from engine import train_one_epoch, train_one_epoch_distillation, evaluate\n",
    "from utils import get_training_dataloader, get_test_dataloader, get_imagenet_train_dataloader, get_imagenet_test_dataloader\n",
    "import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "234c6ce8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating model: vit_tiny_patch16_224\n",
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170498071/170498071 [00:16<00:00, 10475902.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
      "Files already downloaded and verified\n",
      "number of params: 5526346\n"
     ]
    }
   ],
   "source": [
    "MEAN = (0.5070751592371323, 0.48654887331495095, 0.4409178433670343)\n",
    "STD = (0.2673342858792401, 0.2564384629170883, 0.27615047132568404)\n",
    "CHECKPOINT_PATH = './checkpoint'\n",
    "MODEL_NAME = 'vit_base_patch16_224'\n",
    "num_classes = 10\n",
    "EPOCHS = 5\n",
    "LR = 0.0001\n",
    "WD = 0.0\n",
    "shots = 1000\n",
    "\n",
    "print(f\"Creating model: {MODEL_NAME}\")\n",
    "model = create_model(\n",
    "        MODEL_NAME,\n",
    "        pretrained=False,\n",
    "        num_classes=10,\n",
    "        img_size=224)\n",
    "device = 'cuda:0' # device = 'cpu'\n",
    "model = model.to(device)\n",
    "\n",
    "cifar10_training_loader = get_training_dataloader(\n",
    "    MEAN,\n",
    "    STD,\n",
    "    num_workers=2,\n",
    "    batch_size=16,\n",
    "    shuffle=True,\n",
    "    shots=shots\n",
    ")\n",
    "\n",
    "assert (shots*num_classes == len(cifar10_training_loader.dataset))\n",
    "\n",
    "cifar10_test_loader = get_test_dataloader(\n",
    "    MEAN,\n",
    "    STD,\n",
    "    num_workers=4,\n",
    "    batch_size=256,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR, weight_decay=WD)\n",
    "\n",
    "\n",
    "n_parameters = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print('number of params:', n_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f51e2794",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training for 5 epochs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStart training for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mEPOCHS\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m epochs\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, EPOCHS\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m----> 4\u001b[0m     train_stats \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcifar10_training_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m epoch \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m9\u001b[39m: \n\u001b[1;32m      8\u001b[0m         test_stats \u001b[38;5;241m=\u001b[39m evaluate(cifar10_test_loader, model, criterion, device)\n",
      "File \u001b[0;32m~/Documents/GitHub/Vision-Transformer/engine.py:42\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[0;34m(model, criterion, data_loader, optimizer, device, epoch)\u001b[0m\n\u001b[1;32m     39\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     40\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m---> 42\u001b[0m     \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msynchronize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m     metric_logger\u001b[38;5;241m.\u001b[39mupdate(loss\u001b[38;5;241m=\u001b[39mloss_value)\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAveraged stats:\u001b[39m\u001b[38;5;124m\"\u001b[39m, metric_logger)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/cuda/__init__.py:781\u001b[0m, in \u001b[0;36msynchronize\u001b[0;34m(device)\u001b[0m\n\u001b[1;32m    773\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msynchronize\u001b[39m(device: _device_t \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    774\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Waits for all kernels in all streams on a CUDA device to complete.\u001b[39;00m\n\u001b[1;32m    775\u001b[0m \n\u001b[1;32m    776\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    779\u001b[0m \u001b[38;5;124;03m            if :attr:`device` is ``None`` (default).\u001b[39;00m\n\u001b[1;32m    780\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 781\u001b[0m     \u001b[43m_lazy_init\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    782\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mdevice(device):\n\u001b[1;32m    783\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_cuda_synchronize()\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/cuda/__init__.py:289\u001b[0m, in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    284\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    285\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    286\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultiprocessing, you must use the \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspawn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m start method\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    287\u001b[0m     )\n\u001b[1;32m    288\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(torch\u001b[38;5;241m.\u001b[39m_C, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_cuda_getDeviceCount\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 289\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTorch not compiled with CUDA enabled\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    290\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _cudart \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    291\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[1;32m    292\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    293\u001b[0m     )\n",
      "\u001b[0;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "print(f\"Start training for {EPOCHS} epochs\")\n",
    "\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    train_stats = train_one_epoch(\n",
    "        model, criterion, cifar10_training_loader,\n",
    "        optimizer, device, epoch)\n",
    "    if epoch % 10 == 9: \n",
    "        test_stats = evaluate(cifar10_test_loader, model, criterion, device)\n",
    "        print(f\"Accuracy of the network on the {len(cifar10_test_loader)} test images: {test_stats['acc1']:.1f}%\")        \n",
    "        \n",
    "test_stats = evaluate(cifar10_test_loader, model, criterion, device)\n",
    "print(f\"Accuracy of the network on the {len(cifar10_test_loader)} test images: {test_stats['acc1']:.1f}%\")            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d48bd9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate througput \n",
    "start_time = time.time()\n",
    "test_stats = evaluate(cifar10_test_loader, model, criterion, device)\n",
    "end_time = time.time()\n",
    "num_samples = len(cifar10_test_loader.dataset)\n",
    "throughput = num_samples / (end_time - start_time)\n",
    "print(\"Throughput: {}\".format(throughput))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61fdc53f",
   "metadata": {},
   "source": [
    "# Q2 Fine-tuning Pretrained ViT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ee8523be",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The archive ILSVRC2012_devkit_t12.tar.gz is not present in the root directory or is corrupted. You need to download it externally and place it in ./data.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m num_workers \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m4\u001b[39m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Load ImageNet data\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m \u001b[43mget_imagenet_train_dataloader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_workers\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m val_loader \u001b[38;5;241m=\u001b[39m get_imagenet_test_dataloader(batch_size\u001b[38;5;241m=\u001b[39mbatch_size, num_workers\u001b[38;5;241m=\u001b[39mnum_workers)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Define model, criterion, optimizer, etc.\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/GitHub/Vision-Transformer/utils.py:245\u001b[0m, in \u001b[0;36mget_imagenet_train_dataloader\u001b[0;34m(batch_size, num_workers)\u001b[0m\n\u001b[1;32m    231\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\" return ImageNet training dataloader\u001b[39;00m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m    233\u001b[0m \u001b[38;5;124;03m    data_dir: path to ImageNet dataset\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[38;5;124;03mReturns: dataloader: torch dataloader object\u001b[39;00m\n\u001b[1;32m    237\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    238\u001b[0m transform_train \u001b[38;5;241m=\u001b[39m transforms\u001b[38;5;241m.\u001b[39mCompose([\n\u001b[1;32m    239\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mRandomResizedCrop(\u001b[38;5;241m224\u001b[39m),\n\u001b[1;32m    240\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mRandomHorizontalFlip(),\n\u001b[1;32m    241\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mToTensor(),\n\u001b[1;32m    242\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mNormalize(mean\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0.485\u001b[39m, \u001b[38;5;241m0.456\u001b[39m, \u001b[38;5;241m0.406\u001b[39m], std\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0.229\u001b[39m, \u001b[38;5;241m0.224\u001b[39m, \u001b[38;5;241m0.225\u001b[39m]),\n\u001b[1;32m    243\u001b[0m ])\n\u001b[0;32m--> 245\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mtorchvision\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdatasets\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mImageNet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./data\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msplit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransform_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    247\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m DataLoader(\n\u001b[1;32m    248\u001b[0m     train_dataset, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, num_workers\u001b[38;5;241m=\u001b[39mnum_workers, pin_memory\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    250\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m train_loader\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torchvision/datasets/imagenet.py:52\u001b[0m, in \u001b[0;36mImageNet.__init__\u001b[0;34m(self, root, split, **kwargs)\u001b[0m\n\u001b[1;32m     49\u001b[0m root \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexpanduser(root)\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msplit \u001b[38;5;241m=\u001b[39m verify_str_arg(split, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msplit\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m---> 52\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse_archives\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m wnid_to_classes \u001b[38;5;241m=\u001b[39m load_meta_file(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msplit_folder, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torchvision/datasets/imagenet.py:65\u001b[0m, in \u001b[0;36mImageNet.parse_archives\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparse_archives\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m check_integrity(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot, META_FILE)):\n\u001b[0;32m---> 65\u001b[0m         \u001b[43mparse_devkit_archive\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroot\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misdir(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msplit_folder):\n\u001b[1;32m     68\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msplit \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torchvision/datasets/imagenet.py:146\u001b[0m, in \u001b[0;36mparse_devkit_archive\u001b[0;34m(root, file)\u001b[0m\n\u001b[1;32m    143\u001b[0m     file \u001b[38;5;241m=\u001b[39m archive_meta[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    144\u001b[0m md5 \u001b[38;5;241m=\u001b[39m archive_meta[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m--> 146\u001b[0m \u001b[43m_verify_archive\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmd5\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_tmp_dir() \u001b[38;5;28;01mas\u001b[39;00m tmp_dir:\n\u001b[1;32m    149\u001b[0m     extract_archive(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(root, file), tmp_dir)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torchvision/datasets/imagenet.py:102\u001b[0m, in \u001b[0;36m_verify_archive\u001b[0;34m(root, file, md5)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m check_integrity(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(root, file), md5):\n\u001b[1;32m     98\u001b[0m     msg \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     99\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe archive \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m is not present in the root directory or is corrupted. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    100\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou need to download it externally and place it in \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    101\u001b[0m     )\n\u001b[0;32m--> 102\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg\u001b[38;5;241m.\u001b[39mformat(file, root))\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The archive ILSVRC2012_devkit_t12.tar.gz is not present in the root directory or is corrupted. You need to download it externally and place it in ./data."
     ]
    }
   ],
   "source": [
    "MEAN = (0.5070751592371323, 0.48654887331495095, 0.4409178433670343)\n",
    "STD = (0.2673342858792401, 0.2564384629170883, 0.27615047132568404)\n",
    "CHECKPOINT_PATH = './checkpoint'\n",
    "MODEL_NAME = 'vit_base_patch16_224'\n",
    "num_classes = 10\n",
    "EPOCHS = 5\n",
    "LR = 0.0001\n",
    "WD = 0.0\n",
    "shots = 1000\n",
    "\n",
    "print(f\"Creating model: {MODEL_NAME}\")\n",
    "model = create_model(\n",
    "        MODEL_NAME,\n",
    "        pretrained=True,\n",
    "        num_classes=10,\n",
    "        img_size=224)\n",
    "device = 'cuda:0' # device = 'cpu'\n",
    "model = model.to(device)\n",
    "\n",
    "cifar10_training_loader = get_training_dataloader(\n",
    "    MEAN,\n",
    "    STD,\n",
    "    num_workers=2,\n",
    "    batch_size=16,\n",
    "    shuffle=True,\n",
    "    shots=shots\n",
    ")\n",
    "\n",
    "assert (shots*num_classes == len(cifar10_training_loader.dataset))\n",
    "\n",
    "cifar10_test_loader = get_test_dataloader(\n",
    "    MEAN,\n",
    "    STD,\n",
    "    num_workers=4,\n",
    "    batch_size=256,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR, weight_decay=WD)\n",
    "\n",
    "\n",
    "n_parameters = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print('number of params:', n_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f881d0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Start training for {EPOCHS} epochs\")\n",
    "\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    train_stats = train_one_epoch(\n",
    "        model, criterion, cifar10_training_loader,\n",
    "        optimizer, device, epoch)\n",
    "    if epoch % 10 == 9: \n",
    "        test_stats = evaluate(cifar10_test_loader, model, criterion, device)\n",
    "        print(f\"Accuracy of the network on the {len(cifar10_test_loader)} test images: {test_stats['acc1']:.1f}%\")        \n",
    "        \n",
    "test_stats = evaluate(cifar10_test_loader, model, criterion, device)\n",
    "print(f\"Accuracy of the network on the {len(cifar10_test_loader)} test images: {test_stats['acc1']:.1f}%\")            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e323d040",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate througput \n",
    "start_time = time.time()\n",
    "test_stats = evaluate(cifar10_test_loader, model, criterion, device)\n",
    "end_time = time.time()\n",
    "num_samples = len(cifar10_test_loader.dataset)\n",
    "throughput = num_samples / (end_time - start_time)\n",
    "print(\"Throughput: {}\".format(throughput))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a93beb48",
   "metadata": {},
   "source": [
    "# Q3 ViT model on a small device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb0666d",
   "metadata": {},
   "outputs": [],
   "source": [
    "MEAN = (0.5070751592371323, 0.48654887331495095, 0.4409178433670343)\n",
    "STD = (0.2673342858792401, 0.2564384629170883, 0.27615047132568404)\n",
    "CHECKPOINT_PATH = './checkpoint'\n",
    "MODEL_NAME = 'vit_tiny_patch16_224'\n",
    "num_classes = 10\n",
    "EPOCHS = 5\n",
    "LR = 0.0001\n",
    "WD = 0.0\n",
    "shots = 1000\n",
    "\n",
    "print(f\"Creating model: {MODEL_NAME}\")\n",
    "model = create_model(\n",
    "        MODEL_NAME,\n",
    "        pretrained=False,\n",
    "        num_classes=10,\n",
    "        img_size=224)\n",
    "device = 'cuda:0' # device = 'cpu'\n",
    "model = model.to(device)\n",
    "\n",
    "cifar10_training_loader = get_training_dataloader(\n",
    "    MEAN,\n",
    "    STD,\n",
    "    num_workers=2,\n",
    "    batch_size=16,\n",
    "    shuffle=True,\n",
    "    shots=shots\n",
    ")\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR, weight_decay=WD)\n",
    "\n",
    "\n",
    "n_parameters = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print('number of params:', n_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138b86de",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Start training for {EPOCHS} epochs\")\n",
    "\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    train_stats = train_one_epoch(\n",
    "        model, criterion, cifar10_training_loader,\n",
    "        optimizer, device, epoch)\n",
    "    if epoch % 10 == 9: \n",
    "        test_stats = evaluate(cifar10_test_loader, model, criterion, device)\n",
    "        print(f\"Accuracy of the network on the {len(cifar10_test_loader)} test images: {test_stats['acc1']:.1f}%\")        \n",
    "        \n",
    "test_stats = evaluate(cifar10_test_loader, model, criterion, device)\n",
    "print(f\"Accuracy of the network on the {len(cifar10_test_loader)} test images: {test_stats['acc1']:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199e33f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate througput \n",
    "start_time = time.time()\n",
    "test_stats = evaluate(cifar10_test_loader, model, criterion, device)\n",
    "end_time = time.time()\n",
    "num_samples = len(cifar10_test_loader.dataset)\n",
    "throughput = num_samples / (end_time - start_time)\n",
    "print(\"Throughput: {}\".format(throughput))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33816cf2",
   "metadata": {},
   "source": [
    "# Q4 Knowledge Distillation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caaf4f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Train the teacher\n",
    "\n",
    "MODEL_NAME = 'vit_base_patch16_224'\n",
    "num_classes = 10\n",
    "EPOCHS = 5\n",
    "LR = 0.0001\n",
    "WD = 0.0\n",
    "\n",
    "print(f\"Creating model: {MODEL_NAME}\")\n",
    "teacher = create_model(\n",
    "        MODEL_NAME,\n",
    "        pretrained=True,\n",
    "        num_classes=10,\n",
    "        img_size=224)\n",
    "device = 'cuda:0' # device = 'cpu'\n",
    "teacher = teacher.to(device)\n",
    "\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(teacher.parameters(), lr=LR, weight_decay=WD)\n",
    "\n",
    "\n",
    "n_parameters = sum(p.numel() for p in teacher.parameters() if p.requires_grad)\n",
    "print('number of params:', n_parameters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c392f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Start training for {EPOCHS} epochs\")\n",
    "\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    train_stats = train_one_epoch(\n",
    "        teacher, criterion, cifar10_training_loader,\n",
    "        optimizer, device, epoch)\n",
    "    if epoch % 10 == 9: \n",
    "        test_stats = evaluate(cifar10_test_loader, teacher, criterion, device)\n",
    "        print(f\"Accuracy of the network on the {len(cifar10_test_loader)} test images: {test_stats['acc1']:.1f}%\")        \n",
    "        \n",
    "test_stats = evaluate(cifar10_test_loader, teacher, criterion, device)\n",
    "print(f\"Accuracy of the network on the {len(cifar10_test_loader)} test images: {test_stats['acc1']:.1f}%\")       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab509fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save finetuned teacher model\n",
    "torch.save(teacher.state_dict(), './teacher.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69587aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "teacher = create_model(\n",
    "        'vit_base_patch16_224',\n",
    "        pretrained=True,\n",
    "        num_classes=10,\n",
    "        img_size=224)\n",
    "device = 'cuda:0' # device = 'cpu'\n",
    "teacher = teacher.to(device)\n",
    "teacher.load_state_dict(torch.load('./teacher.pth'))\n",
    "\n",
    "test_stats = evaluate(cifar10_test_loader, teacher, criterion, device)\n",
    "print(f\"Accuracy of the network on the {len(cifar10_test_loader)} test images: {test_stats['acc1']:.1f}%\")            \n",
    "\n",
    "# Train the student\n",
    "for p in teacher.parameters(): \n",
    "    p.requires_grad = False\n",
    "\n",
    "MODEL_NAME = 'vit_tiny_patch16_224'\n",
    "\n",
    "model = create_model(\n",
    "        MODEL_NAME,\n",
    "        pretrained=True,\n",
    "        num_classes=10,\n",
    "        img_size=224)\n",
    "device = 'cuda:0' # device = 'cpu'\n",
    "model = model.to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR, weight_decay=WD)\n",
    "\n",
    "\n",
    "n_parameters = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print('number of params:', n_parameters)\n",
    "\n",
    "\n",
    "print(f\"Start training for {EPOCHS} epochs\")\n",
    "\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    train_stats = train_one_epoch_distillation(\n",
    "        teacher, model, criterion, cifar10_training_loader,\n",
    "        optimizer, device, epoch, alpha=2.0, temp=1.0)\n",
    "    if epoch % 2 == 1: \n",
    "        test_stats = evaluate(cifar10_test_loader, model, criterion, device)\n",
    "        print(f\"Accuracy of the network on the {len(cifar10_test_loader)} test images: {test_stats['acc1']:.1f}%\")        \n",
    "        \n",
    "test_stats = evaluate(cifar10_test_loader, model, criterion, device)\n",
    "print(f\"Accuracy of the network on the {len(cifar10_test_loader)} test images: {test_stats['acc1']:.1f}%\")            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
